"""
clean_equipment_claims.py
=========================
Data cleaning pipeline for equipment_failure_claims_freq.csv and
equipment_failure_claims_sev.csv, guided by requirements.xlsx.

Pipeline order
--------------
1.   Load both CSVs (semicolon-delimited, comma decimal)
2.   Standardise column names

     -- policy_id is cleaned and imputed FIRST so that all subsequent
        cross-imputation steps can rely on a clean, populated key --

3.   Cross-impute policy_id between datasets
         direct match on equipment_id (freq <-> sev)
4.   Impute still-missing policy_id via secondary keys
         Key 1: equipment_id + exposure  (rounded to 4 dp)
         Key 2: equipment_id + equipment_type  (categorical fallback)
5.   Fill remaining missing policy_id  -> MP-0001, MP-0002 ...
     Fill remaining missing equipment_id -> ME-0001, ME-0002 ...

     -- now that policy_id is reliable, clean and impute everything else --

6.   Fix corrupted categorical values  (strip _???XXXX suffixes)
7.   Cross-impute shared columns between datasets (key: policy_id + equipment_id)
8.   Convert negatives to NaN for all non-negative fields
9.   Fix out-of-range exposure values (must be 0-1)
10.  Fix corrupted claim_count / claim_seq  (non-integer floats -> NaN)
     Re-derive claim_count = COUNT of sev rows per policy_id
     (not max(claim_seq) — row count is the correct definition)
11.  Cap equipment_age, maintenance_int, usage_int at 99th percentile
11a. Drop sev rows where claim_amount exceeds the 99th percentile
12.  Cast claim_count and claim_seq to nullable integer
13.  Apply per-column fallback strategies for any remaining NaN
14.  Save cleaned CSVs
15.  Generate Markdown audit report

Author : Claude (Anthropic)
"""

# =============================================================================
# USER CONFIGURATION
# =============================================================================
# Edit strategies for any column still containing NaN after all imputation.
# Run once first — the report shows exactly how many NaN remain per column.
#
# Strategy options:
#   "flag"    -> keep NaN, add boolean column <col>_missing   (DEFAULT)
#   "drop"    -> drop rows where this column is still NaN
#   "median"  -> fill with column median           (numeric only)
#   "mode"    -> fill with most-frequent value     (categorical or numeric)
#   "zero"    -> fill with 0                       (numeric only)
#   "unknown" -> fill with "UNKNOWN"               (categorical only)
# =============================================================================

UNRESOLVABLE_STRATEGY = {
    # policy_id and equipment_id are never listed here — always resolved
    # in Steps 3-5 before any other strategy is needed.
    # -- FREQ & SEV shared ------------------------------------------------
    "equipment_type":  "mode",
    "equipment_age":   "median",
    "solar_system":    "mode",
    "maintenance_int": "median",
    "usage_int":       "median",
    "exposure":        "median",
    # -- FREQ only --------------------------------------------------------
    "claim_count":     "zero",
    # -- SEV only ---------------------------------------------------------
    "claim_id":        "unknown",
    "claim_seq":       "zero",
    "claim_amount":    "drop",
}

# =============================================================================
# INPUT / OUTPUT PATHS
# =============================================================================
FREQ_IN  = r"data_clean\test_equipment_fail\equipment_failure_claims_freq.csv"
SEV_IN   = r"data_clean\test_equipment_fail\equipment_failure_claims_sev.csv"
FREQ_OUT = "cleaned_freq.csv"
SEV_OUT  = "cleaned_sev.csv"
REPORT   = "cleaning_report.md"

# =============================================================================
# Imports
# =============================================================================
import re
import sys
from datetime import datetime
from pathlib import Path

import numpy as np
import pandas as pd

# =============================================================================
# Canonical reference values  (from requirements.xlsx + data inspection)
# =============================================================================

# NOTE: requirements.xlsx contained two typos:
#   "Quantam Bore"     -> correct: "Quantum Bore"
#   "Regl Aggregators" -> correct: "ReglAggregators"  (as seen in raw data)
VALID_EQUIPMENT_TYPES = {
    "FexStram Carrier",
    "Flux Rider",
    "Graviton Extractor",
    "Ion Pulverizer",
    "Quantum Bore",
    "ReglAggregators",
}

VALID_SOLAR_SYSTEMS = {"Helionis Cluster", "Epsilon", "Zeta"}

# Columns that must be >= 0
NON_NEGATIVE_COLS = {
    "freq": ["equipment_age", "maintenance_int", "usage_int", "exposure", "claim_count"],
    "sev":  ["equipment_age", "maintenance_int", "usage_int", "exposure", "claim_amount"],
}

# Columns to cap at the 99th percentile
# claim_amount is NOT here — rows above its 99th pct are dropped (Step 11a)
CAP_COLS = {
    "freq": ["equipment_age", "maintenance_int", "usage_int"],
    "sev":  ["equipment_age", "maintenance_int", "usage_int"],
}

# Decimal places to round exposure before using it as a join key
_EXP_ROUND = 4

# =============================================================================
# Audit-log infrastructure
# =============================================================================

_LOG: list[dict] = []
_RAW_LOG: list[str] = []


def record(msg: str, step: str = "", dataset: str = "", column: str = "",
           issue: str = "", action: str = "", n: int = 0) -> None:
    ts = datetime.now().strftime("%H:%M:%S")
    line = f"[{ts}]  {msg}"
    print(line)
    _RAW_LOG.append(line)
    if step:
        _LOG.append(dict(step=step, dataset=dataset, column=column,
                         issue=issue, action=action, n=n))


def null_count(df: pd.DataFrame) -> dict:
    return df.isnull().sum().to_dict()


def section(title: str) -> None:
    record("")
    record("=" * 68)
    record(title)
    record("=" * 68)


# =============================================================================
# Helper: Markdown table builder
# =============================================================================

def md_table(headers: list, rows: list) -> str:
    if not rows:
        return "_No data._"
    sep  = "| " + " | ".join(["---"] * len(headers)) + " |"
    head = "| " + " | ".join(str(h) for h in headers) + " |"
    body = "\n".join("| " + " | ".join(str(c) for c in row) + " |" for row in rows)
    return "\n".join([head, sep, body])


# =============================================================================
# Helper: apply per-column fallback strategies (Step 13)
# =============================================================================

def apply_unresolvable(df: pd.DataFrame, dataset_name: str) -> pd.DataFrame:
    for col, strategy in UNRESOLVABLE_STRATEGY.items():
        if col not in df.columns:
            continue
        remaining = int(df[col].isna().sum())
        if remaining == 0:
            continue

        is_numeric = pd.api.types.is_numeric_dtype(df[col])

        record(
            f"  [{dataset_name}] '{col}': {remaining} NaN remain "
            f"-> strategy = '{strategy}'",
            step="Step 13", dataset=dataset_name, column=col,
            issue=f"{remaining} unresolvable NaN",
            action=strategy, n=remaining,
        )

        if strategy == "flag":
            df[f"{col}_missing"] = df[col].isna().astype("boolean")

        elif strategy == "drop":
            before = len(df)
            df = df.dropna(subset=[col])
            record(f"    Dropped {before - len(df)} rows")

        elif strategy == "median":
            if not is_numeric:
                record(f"  WARNING: 'median' invalid for non-numeric '{col}' "
                       f"-> falling back to 'flag'")
                df[f"{col}_missing"] = df[col].isna().astype("boolean")
            else:
                fill_val = df[col].median()
                df[col] = df[col].fillna(fill_val)
                record(f"    Filled with median = {fill_val:.4f}")

        elif strategy == "mode":
            if df[col].dropna().empty:
                record(f"  WARNING: no non-null values for mode on '{col}' -> skipped")
            else:
                fill_val = df[col].mode().iloc[0]
                try:
                    df[col] = df[col].fillna(fill_val)
                    record(f"    Filled with mode = {fill_val}")
                except TypeError:
                    record(f"  WARNING: mode fill failed for '{col}' "
                           f"(dtype {df[col].dtype}) -> falling back to 'flag'")
                    df[f"{col}_missing"] = df[col].isna().astype("boolean")

        elif strategy == "zero":
            if not is_numeric:
                record(f"  WARNING: 'zero' invalid for non-numeric '{col}' "
                       f"-> falling back to 'flag'")
                df[f"{col}_missing"] = df[col].isna().astype("boolean")
            else:
                df[col] = df[col].fillna(0)
                record("    Filled with 0")

        elif strategy == "unknown":
            if is_numeric:
                record(f"  WARNING: 'unknown' invalid for numeric '{col}' "
                       f"-> falling back to 'flag'")
                df[f"{col}_missing"] = df[col].isna().astype("boolean")
            else:
                df[col] = df[col].fillna("UNKNOWN")
                record("    Filled with 'UNKNOWN'")

        else:
            record(f"  WARNING: unrecognised strategy '{strategy}' for '{col}' — skipped")

    return df


# =============================================================================
# STEP 1 — Load
# =============================================================================
section("STEP 1 — Loading raw datasets")

for path in [FREQ_IN, SEV_IN]:
    if not Path(path).exists():
        sys.exit(f"ERROR: File not found -> {path}")

freq = pd.read_csv(FREQ_IN, sep=";", decimal=",")
sev  = pd.read_csv(SEV_IN,  sep=";", decimal=",")

raw_nulls_freq = null_count(freq)
raw_nulls_sev  = null_count(sev)
raw_rows_freq  = len(freq)
raw_rows_sev   = len(sev)
raw_cols_freq  = freq.shape[1]
raw_cols_sev   = sev.shape[1]

record(f"  freq loaded : {len(freq):,} rows x {freq.shape[1]} columns")
record(f"  sev  loaded : {len(sev):,} rows  x {sev.shape[1]} columns")

# =============================================================================
# STEP 2 — Standardise column names
# =============================================================================
section("STEP 2 — Standardising column names")

freq.columns = freq.columns.str.strip()
sev.columns  = sev.columns.str.strip()

record("  freq columns : " + ", ".join(freq.columns.tolist()))
record("  sev  columns : " + ", ".join(sev.columns.tolist()))

# =============================================================================
# STEP 3 — Cross-impute policy_id between datasets  (direct equipment_id match)
# =============================================================================
# This is the first and highest-priority policy_id recovery step.
# Logic: if a row in dataset A has a known policy_id for a given equipment_id,
# and dataset B has a row with the same equipment_id but NaN policy_id,
# fill it in from A.
#
# A match is only used when it is unambiguous — i.e. equipment_id maps to
# exactly one unique policy_id in the source dataset.
# =============================================================================
section("STEP 3 — Cross-imputing policy_id between datasets (key: equipment_id)")
record("  Rule: only fill when equipment_id maps to exactly 1 unique policy_id")


def build_pid_from_equip(source: pd.DataFrame) -> dict:
    """
    Return dict {equipment_id: policy_id} for equipment_ids that map to
    exactly one unique policy_id in source (unambiguous keys only).
    """
    src = source.dropna(subset=["policy_id", "equipment_id"])
    counts = src.groupby("equipment_id")["policy_id"].nunique()
    unambiguous = set(counts[counts == 1].index)
    lookup = (
        src[src["equipment_id"].isin(unambiguous)]
        .groupby("equipment_id")["policy_id"]
        .first()
        .to_dict()
    )
    return lookup


for target, t_name, source, s_name in [
    (freq, "freq", sev,  "sev"),
    (sev,  "sev",  freq, "freq"),
]:
    null_mask = target["policy_id"].isna() & target["equipment_id"].notna()
    if not null_mask.any():
        record(f"  [{t_name}] policy_id: no missing values — skipping")
        continue

    lookup = build_pid_from_equip(source)
    filled_idx = target.index[null_mask]
    n = 0
    for idx in filled_idx:
        eid = target.at[idx, "equipment_id"]
        pid = lookup.get(eid)
        if pid is not None:
            target.at[idx, "policy_id"] = pid
            n += 1

    still = int(target["policy_id"].isna().sum())
    if n:
        record(
            f"  [{t_name} <- {s_name}] policy_id: {n} filled via equipment_id match "
            f"| {still} still missing",
            step="Step 3", dataset=t_name, column="policy_id",
            issue="policy_id NaN — equipment_id match found in other dataset",
            action="filled from unambiguous equipment_id lookup",
            n=n,
        )
    else:
        record(f"  [{t_name}] policy_id: no unambiguous equipment_id matches "
               f"({still} still missing)")

    if t_name == "freq":
        freq = target
    else:
        sev = target

# =============================================================================
# STEP 4 — Impute still-missing policy_id via secondary key combinations
# =============================================================================
# For rows where policy_id is still NaN after Step 3, try to recover it
# using combinations of other columns as a composite lookup key.
#
# Keys tried in order (most to least selective):
#   1. equipment_id + exposure        (exposure rounded to 4 dp)
#   2. equipment_id + equipment_type  (categorical fallback)
#
# Note: equipment_type has not been cleaned yet at this stage, so corrupted
# values (e.g. "Flux Rider_???1234") may reduce the match rate for Key 2.
# Any policy_ids not recoverable here will receive MP-#### placeholders.
# =============================================================================
section("STEP 4 — Imputing missing policy_id via secondary key combinations")
record("  Key 1: equipment_id + exposure  (rounded to 4 dp)")
record("  Key 2: equipment_id + equipment_type  (categorical fallback)")
record("  Rule : only fill when key maps to exactly 1 unique policy_id")

duplicates = freq["policy_id"].duplicated().sum()
if duplicates > 0:
    print(f"CRITICAL: Found {duplicates} duplicate policy_ids in freq!")
def build_secondary_pid_lookup(source: pd.DataFrame, key_cols: list) -> dict:
    """
    Build {tuple(key_values): policy_id} for unambiguous keys only.
    Adds a rounded exposure helper column if needed.
    """
    src = source.dropna(subset=["policy_id"] + key_cols).copy()
    if "exposure_r" in key_cols:
        src["exposure_r"] = src["exposure"].round(_EXP_ROUND)

    counts = src.groupby(key_cols)["policy_id"].nunique()
    unambiguous = set(counts[counts == 1].index.tolist())

    lookup = {}
    for row in src.itertuples(index=False):
        key = tuple(getattr(row, c) for c in key_cols)
        if key in unambiguous:
            lookup[key] = row.policy_id
    return lookup


def apply_secondary_pid(target: pd.DataFrame, lookup: dict,
                         key_cols: list) -> tuple[pd.DataFrame, int]:
    """Fill NaN policy_id rows using composite key lookup."""
    null_mask = target["policy_id"].isna()
    if not null_mask.any():
        return target, 0

    if "exposure_r" in key_cols:
        target = target.copy()
        target["exposure_r"] = target["exposure"].round(_EXP_ROUND)

    n = 0
    for idx in target.index[null_mask]:
        row = target.loc[idx]
        if any(pd.isna(row.get(kc, np.nan)) for kc in key_cols):
            continue
        key = tuple(row[kc] for kc in key_cols)
        pid = lookup.get(key)
        if pid is not None:
            target.at[idx, "policy_id"] = pid
            n += 1

    if "exposure_r" in target.columns:
        target = target.drop(columns=["exposure_r"])

    return target, n


for target, t_name, source, s_name in [
    (freq, "freq", sev,  "sev"),
    (sev,  "sev",  freq, "freq"),
]:
    if target["policy_id"].isna().sum() == 0:
        record(f"  [{t_name}] policy_id: no missing values — skipping")
        continue

    total = 0

    # Key 1: equipment_id + exposure
    src_exp = source.copy()
    src_exp["exposure_r"] = src_exp["exposure"].round(_EXP_ROUND)
    lkp1 = build_secondary_pid_lookup(src_exp, ["equipment_id", "exposure_r"])
    target, n1 = apply_secondary_pid(target, lkp1, ["equipment_id", "exposure_r"])
    if n1:
        record(
            f"  [{t_name} <- {s_name}] policy_id: {n1} filled "
            f"via (equipment_id + exposure)",
            step="Step 4", dataset=t_name, column="policy_id",
            issue="policy_id NaN — secondary key match found",
            action="imputed via equipment_id + exposure",
            n=n1,
        )
        total += n1

    # Key 2: equipment_id + equipment_type  (fallback)
    if target["policy_id"].isna().sum() > 0:
        lkp2 = build_secondary_pid_lookup(source, ["equipment_id", "equipment_type"])
        target, n2 = apply_secondary_pid(target, lkp2, ["equipment_id", "equipment_type"])
        if n2:
            record(
                f"  [{t_name} <- {s_name}] policy_id: {n2} filled "
                f"via (equipment_id + equipment_type)",
                step="Step 4", dataset=t_name, column="policy_id",
                issue="policy_id NaN — secondary key match found",
                action="imputed via equipment_id + equipment_type",
                n=n2,
            )
            total += n2

    still = int(target["policy_id"].isna().sum())
    if total == 0:
        record(f"  [{t_name}] policy_id: no secondary key matches found "
               f"({still} still missing -> MP-#### in Step 5)")
    else:
        record(f"  [{t_name}] policy_id: {total} recovered | "
               f"{still} remain (-> MP-#### in Step 5)")

    if t_name == "freq":
        freq = target
    else:
        sev = target

# =============================================================================
# STEP 5 — Assign sequential placeholders for still-missing IDs
# =============================================================================
# Last resort after all real-data recovery attempts. Guarantees every row
# has a non-null policy_id and equipment_id for downstream joins.
#
#   Missing policy_id   -> MP-0001, MP-0002 ...  (MP = Missing Policy)
#   Missing equipment_id -> ME-0001, ME-0002 ...  (ME = Missing Equipment)
#
# Counters are shared across both datasets — no collisions possible.
# =============================================================================
section("STEP 5 — Assigning placeholder IDs for still-missing policy_id / equipment_id")
record("  policy_id    placeholders: MP-0001, MP-0002, ...")
record("  equipment_id placeholders: ME-0001, ME-0002, ...")

_mp_counter = 1
_me_counter = 1

for df, name in [(freq, "freq"), (sev, "sev")]:

    # policy_id
    pid_null = df["policy_id"].isna()
    n_pid = int(pid_null.sum())
    if n_pid:
        phs = [f"MP-{_mp_counter + i:04d}" for i in range(n_pid)]
        df.loc[pid_null, "policy_id"] = phs
        _mp_counter += n_pid
        record(
            f"  [{name}] policy_id   : {n_pid} NaN -> {phs[0]} ... {phs[-1]}",
            step="Step 5", dataset=name, column="policy_id",
            issue="policy_id unrecoverable after all imputation",
            action="assigned sequential placeholder MP-####",
            n=n_pid,
        )
    else:
        record(f"  [{name}] policy_id   : fully populated — no placeholders needed")

    # equipment_id
    eid_null = df["equipment_id"].isna()
    n_eid = int(eid_null.sum())
    if n_eid:
        ehs = [f"ME-{_me_counter + i:04d}" for i in range(n_eid)]
        df.loc[eid_null, "equipment_id"] = ehs
        _me_counter += n_eid
        record(
            f"  [{name}] equipment_id: {n_eid} NaN -> {ehs[0]} ... {ehs[-1]}",
            step="Step 5", dataset=name, column="equipment_id",
            issue="equipment_id unrecoverable after all imputation",
            action="assigned sequential placeholder ME-####",
            n=n_eid,
        )
    else:
        record(f"  [{name}] equipment_id: fully populated — no placeholders needed")

# =============================================================================
# STEP 6 — Fix corrupted categorical values  (_???XXXX noise)
# =============================================================================
section("STEP 6 — Fixing corrupted categorical values (_???XXXX noise)")
record("  Rule: strip suffix; set non-canonical residuals to NaN")

_CORRUPTION_RE = re.compile(r"_\?\?\?\d{4}$")


def strip_corruption(series: pd.Series) -> pd.Series:
    return series.str.replace(_CORRUPTION_RE, "", regex=True).str.strip()


for df, name in [(freq, "freq"), (sev, "sev")]:

    # equipment_type
    before = int(df["equipment_type"].isna().sum())
    df["equipment_type"] = strip_corruption(df["equipment_type"])
    bad = df["equipment_type"].notna() & ~df["equipment_type"].isin(VALID_EQUIPMENT_TYPES)
    n = int(bad.sum())
    df.loc[bad, "equipment_type"] = np.nan
    after = int(df["equipment_type"].isna().sum())
    record(
        f"  [{name}] equipment_type: {n} non-canonical cleared | NaN {before} -> {after}",
        step="Step 6", dataset=name, column="equipment_type",
        issue="_???XXXX corruption or non-canonical label",
        action="strip suffix; set non-canonical to NaN",
        n=n,
    )

    # solar_system
    before = int(df["solar_system"].isna().sum())
    df["solar_system"] = strip_corruption(df["solar_system"])
    bad = df["solar_system"].notna() & ~df["solar_system"].isin(VALID_SOLAR_SYSTEMS)
    n = int(bad.sum())
    df.loc[bad, "solar_system"] = np.nan
    after = int(df["solar_system"].isna().sum())
    record(
        f"  [{name}] solar_system  : {n} non-canonical cleared | NaN {before} -> {after}",
        step="Step 6", dataset=name, column="solar_system",
        issue="_???XXXX corruption or non-canonical label",
        action="strip suffix; set non-canonical to NaN",
        n=n,
    )

# =============================================================================
# STEP 7 — Cross-impute shared columns between datasets
# =============================================================================
# Now that policy_id is clean and fully populated, the (policy_id, equipment_id)
# join key is reliable for maximum cross-imputation coverage.
# =============================================================================
section("STEP 7 — Cross-imputing shared columns (key: policy_id + equipment_id)")
record("  Columns: equipment_type, equipment_age, solar_system, "
       "maintenance_int, usage_int, exposure")

SHARED_COLS = [
    "equipment_type", "equipment_age", "solar_system",
    "maintenance_int", "usage_int", "exposure",
]


def build_shared_lookup(df: pd.DataFrame) -> pd.DataFrame:
    """De-duplicated lookup: first non-null value per (policy_id, equipment_id) key."""
    key_cols = ["policy_id", "equipment_id"]
    cols = key_cols + [c for c in SHARED_COLS if c in df.columns]
    return (
        df[cols]
        .groupby(key_cols, dropna=False)
        .first()
        .reset_index()
    )


def cross_impute_shared(target: pd.DataFrame, source_lookup: pd.DataFrame,
                         t_name: str, s_name: str) -> pd.DataFrame:
    key_cols = ["policy_id", "equipment_id"]
    fill_cols = [c for c in SHARED_COLS
                 if c in target.columns and c in source_lookup.columns]

    merged = target.merge(
        source_lookup[key_cols + fill_cols],
        on=key_cols, how="left", suffixes=("", "_src"),
    )

    total = 0
    for col in fill_cols:
        src = f"{col}_src"
        if src not in merged.columns:
            continue
        mask = merged[col].isna() & merged[src].notna()
        n = int(mask.sum())
        if n:
            merged.loc[mask, col] = merged.loc[mask, src]
            record(
                f"  [{t_name} <- {s_name}] '{col}': filled {n} NaN values",
                step="Step 7", dataset=t_name, column=col,
                issue="NaN — value available in other dataset",
                action=f"cross-imputed from {s_name}",
                n=n,
            )
            total += n

    merged = merged.drop(columns=[f"{c}_src" for c in fill_cols
                                    if f"{c}_src" in merged.columns])
    if total == 0:
        record(f"  [{t_name} <- {s_name}] No additional cross-imputation possible")
    return merged


freq_lkp = build_shared_lookup(freq)
sev_lkp  = build_shared_lookup(sev)

freq = cross_impute_shared(freq, sev_lkp,  "freq", "sev")
sev  = cross_impute_shared(sev,  freq_lkp, "sev",  "freq")

# =============================================================================
# STEP 8 — Convert negatives to NaN
# =============================================================================
section("STEP 8 — Converting invalid negative values to NaN")
record("  Rule: all listed fields must be >= 0")

for df, name in [(freq, "freq"), (sev, "sev")]:
    for col in NON_NEGATIVE_COLS[name]:
        if col not in df.columns:
            continue
        mask = df[col] < 0
        n = int(mask.sum())
        if n:
            df.loc[mask, col] = np.nan
            record(
                f"  [{name}] '{col}': {n} negative values -> NaN",
                step="Step 8", dataset=name, column=col,
                issue="negative in non-negative field",
                action="replaced with NaN",
                n=n,
            )

# =============================================================================
# STEP 9 — Fix out-of-range exposure  (valid range: 0 to 1)
# =============================================================================
section("STEP 9 — Fixing out-of-range exposure values (valid range: 0 to 1)")

step9_details: dict = {}

for df, name in [(freq, "freq"), (sev, "sev")]:
    over = df["exposure"].notna() & (df["exposure"] > 1)
    n_over = int(over.sum())
    rescued = still_bad = 0

    if n_over:
        def rescale(val):
            if pd.isna(val) or val <= 1:
                return val
            for d in (10, 100):
                c = round(val / d, 6)
                if 0 <= c <= 1:
                    return c
            return np.nan

        df.loc[over, "exposure"] = df.loc[over, "exposure"].apply(rescale)
        still_bad = int((df["exposure"] > 1).sum())
        rescued = n_over - still_bad
        df.loc[df["exposure"] > 1, "exposure"] = np.nan

        record(
            f"  [{name}] exposure: {n_over} values > 1 | "
            f"{rescued} rescaled (div 10/100) | {still_bad} -> NaN",
            step="Step 9", dataset=name, column="exposure",
            issue="exposure outside [0, 1]",
            action="rescale div10/div100 where possible, else NaN",
            n=n_over,
        )

    step9_details[name] = dict(total=n_over, rescued=rescued, nulled=still_bad)

# =============================================================================
# STEP 10 — Fix claim_count and claim_seq
# =============================================================================
section("STEP 10 — Fixing claim_count (freq) and claim_seq (sev)")

# -- 10a. Non-integer floats -> NaN -------------------------------------------
def null_non_integers(series: pd.Series) -> tuple[pd.Series, int]:
    mask = series.notna() & (series != series.round(0))
    n = int(mask.sum())
    s = series.copy()
    s[mask] = np.nan
    return s, n

freq["claim_count"], n_cc = null_non_integers(freq["claim_count"])
record(
    f"  [freq] claim_count: {n_cc} non-integer floats -> NaN",
    step="Step 10a", dataset="freq", column="claim_count",
    issue="non-integer float in count field",
    action="set to NaN", n=n_cc,
)

sev["claim_seq"], n_cs = null_non_integers(sev["claim_seq"])
record(
    f"  [sev] claim_seq: {n_cs} non-integer floats -> NaN",
    step="Step 10a", dataset="sev", column="claim_seq",
    issue="non-integer float in sequence field",
    action="set to NaN", n=n_cs,
)

# -- 10b. Negative claim_count / claim_seq -> NaN (catch-all) -----------------
for df, name, col in [(freq, "freq", "claim_count"), (sev, "sev", "claim_seq")]:
    mask = df[col].notna() & (df[col] < 0)
    n = int(mask.sum())
    if n:
        df.loc[mask, col] = np.nan
        record(
            f"  [{name}] {col}: {n} negative values -> NaN",
            step="Step 10b", dataset=name, column=col,
            issue="negative count / sequence number",
            action="set to NaN", n=n,
        )

# -- 10c. Derive claim_count = COUNT of sev rows per policy_id ----------------
# Definition (from requirements):
#   claim_count for a policy_id = number of rows in sev for that policy_id.
#
# This is a direct row count — NOT max(claim_seq).
# max(claim_seq) would give the wrong answer if claim_seq has gaps, duplicates,
# or NaN values. Row count is always correct.
record("  [freq] Deriving claim_count = COUNT(sev rows) per policy_id ...")

sev_row_counts = (
    sev.dropna(subset=["policy_id"])
    .groupby("policy_id")
    .size()
    .rename("_sev_count")
)

freq = freq.merge(sev_row_counts, on="policy_id", how="left")

# Fill NaN claim_counts (policies with no sev rows get 0)
derived_mask = freq["claim_count"].isna() & freq["_sev_count"].notna()
n_derived = int(derived_mask.sum())
freq.loc[derived_mask, "claim_count"] = freq.loc[derived_mask, "_sev_count"]

# Policies not in sev at all -> claim_count = 0
no_sev_mask = freq["claim_count"].isna() & freq["_sev_count"].isna()
n_zero = int(no_sev_mask.sum())
freq.loc[no_sev_mask, "claim_count"] = 0

freq.drop(columns=["_sev_count"], inplace=True)

record(
    f"  [freq] claim_count: {n_derived} derived from sev row count | "
    f"{n_zero} set to 0 (no sev rows for policy)",
    step="Step 10c", dataset="freq", column="claim_count",
    issue="claim_count NaN or unverified",
    action="re-derived as COUNT(sev rows) per policy_id",
    n=n_derived + n_zero,
)

# -- 10d. Correct claim_count mismatches vs actual sev row count --------------
# Even rows that already had a claim_count value are verified against the
# true sev row count and corrected if they disagree.
record("  [freq] Verifying all claim_count values against sev row counts ...")

freq = freq.merge(sev_row_counts, on="policy_id", how="left")
freq["_sev_count"] = freq["_sev_count"].fillna(0)

mismatch_mask = (
    freq["claim_count"].notna()
    & (freq["claim_count"] != freq["_sev_count"])
)
n_mismatch = int(mismatch_mask.sum())
if n_mismatch:
    freq.loc[mismatch_mask, "claim_count"] = freq.loc[mismatch_mask, "_sev_count"]
    record(
        f"  [freq] claim_count: {n_mismatch} values corrected to match sev row count",
        step="Step 10d", dataset="freq", column="claim_count",
        issue="claim_count disagreed with actual sev row count",
        action="overwritten with COUNT(sev rows)",
        n=n_mismatch,
    )

freq.drop(columns=["_sev_count"], inplace=True)

# =============================================================================
# STEP 11 — Cap continuous variables at 99th percentile
# =============================================================================
section("STEP 11 — Capping continuous variables at 99th percentile")
record("  Columns: equipment_age, maintenance_int, usage_int (both datasets)")
record("  Percentile computed on non-null, non-negative values only")

cap_log: dict = {}

for df, name in [(freq, "freq"), (sev, "sev")]:
    cap_log[name] = {}
    for col in CAP_COLS[name]:
        if col not in df.columns:
            continue
        valid = df[col].dropna()
        valid = valid[valid >= 0]
        if valid.empty:
            continue
        cap_val = float(valid.quantile(0.99))
        over = df[col].notna() & (df[col] > cap_val)
        n = int(over.sum())
        df.loc[over, col] = cap_val
        cap_log[name][col] = cap_val
        record(
            f"  [{name}] '{col}': 99th pct = {cap_val:.4f} | {n} values capped",
            step="Step 11", dataset=name, column=col,
            issue="value above 99th percentile",
            action=f"capped at {cap_val:.4f}",
            n=n,
        )

# =============================================================================
# STEP 11a — Drop sev rows where claim_amount exceeds the 99th percentile
# =============================================================================
# Capping would distort actuarial loss distributions. Rows above the 99th
# percentile are dropped so the modelled severity reflects real observed losses.
# =============================================================================
section("STEP 11a — Dropping sev rows where claim_amount > 99th percentile")

sev["claim_amount"] = pd.to_numeric(sev["claim_amount"], errors="coerce")

_valid_amt = sev["claim_amount"].dropna()
_valid_amt = _valid_amt[_valid_amt >= 0]

_amt_cap = None
_amt_dropped_ids: list = []
_amt_dropped_n = 0

if _valid_amt.empty:
    record("  [sev] claim_amount: no valid values — skipping")
else:
    _amt_cap = float(_valid_amt.quantile(0.99))
    drop_mask = sev["claim_amount"].notna() & (sev["claim_amount"] > _amt_cap)
    _amt_dropped_n = int(drop_mask.sum())
    _amt_dropped_ids = (
        sev.loc[drop_mask, "claim_id"].tolist()
        if "claim_id" in sev.columns else []
    )

    sev = sev[~drop_mask].reset_index(drop=True)

    record(
        f"  [sev] claim_amount: threshold = {_amt_cap:,.4f} | "
        f"{_amt_dropped_n} rows dropped | {len(sev):,} rows remain",
        step="Step 11a", dataset="sev", column="claim_amount",
        issue="claim_amount above 99th percentile",
        action=f"row dropped (threshold = {_amt_cap:,.4f})",
        n=_amt_dropped_n,
    )
    if _amt_dropped_ids:
        sample = ", ".join(str(x) for x in _amt_dropped_ids[:10])
        suffix = f" ... (+{len(_amt_dropped_ids)-10} more)" if len(_amt_dropped_ids) > 10 else ""
        record(f"  [sev] dropped claim_ids: {sample}{suffix}")

# After dropping rows from sev, re-sync claim_count in freq
# so it still reflects the correct number of retained sev rows.
record("  [freq] Re-syncing claim_count after sev row drops ...")

sev_row_counts_post = (
    sev.dropna(subset=["policy_id"])
    .groupby("policy_id")
    .size()
    .rename("_sev_count_post")
)

freq = freq.merge(sev_row_counts_post, on="policy_id", how="left")
freq["_sev_count_post"] = freq["_sev_count_post"].fillna(0)

resync_mask = freq["claim_count"] != freq["_sev_count_post"]
n_resync = int(resync_mask.sum())
if n_resync:
    freq.loc[resync_mask, "claim_count"] = freq.loc[resync_mask, "_sev_count_post"]
    record(
        f"  [freq] claim_count: {n_resync} values updated to reflect dropped sev rows",
        step="Step 11a", dataset="freq", column="claim_count",
        issue="claim_count no longer matches sev row count after drops",
        action="re-synced to COUNT(retained sev rows)",
        n=n_resync,
    )

freq.drop(columns=["_sev_count_post"], inplace=True)

# =============================================================================
# STEP 12 — Cast claim_count and claim_seq to nullable integer
# =============================================================================
section("STEP 12 — Casting claim_count and claim_seq to Int64")

freq["claim_count"] = (
    pd.to_numeric(freq["claim_count"], errors="coerce")
    .round(0)
    .astype("Int64")
)
sev["claim_seq"] = (
    pd.to_numeric(sev["claim_seq"], errors="coerce")
    .round(0)
    .astype("Int64")
)

record("  [freq] claim_count -> Int64")
record("  [sev]  claim_seq   -> Int64")
record("  [sev]  claim_amount already numeric (float64)")

# =============================================================================
# STEP 13 — Apply per-column fallback strategies for remaining NaN
# =============================================================================
section("STEP 13 — Applying fallback strategies for remaining NaN values")

final_nulls_before_freq = null_count(freq)
final_nulls_before_sev  = null_count(sev)

freq = apply_unresolvable(freq, "freq")
sev  = apply_unresolvable(sev,  "sev")

# =============================================================================
# STEP 14 — Save cleaned datasets
# =============================================================================
section("STEP 14 — Saving cleaned datasets")

freq.to_csv(FREQ_OUT, sep=";", decimal=".", index=False)
sev.to_csv(SEV_OUT,   sep=";", decimal=".", index=False)

record(f"  Saved -> {FREQ_OUT}  ({len(freq):,} rows x {freq.shape[1]} cols)")
record(f"  Saved -> {SEV_OUT}   ({len(sev):,} rows  x {sev.shape[1]} cols)")

# =============================================================================
# STEP 15 — Generate Markdown audit report
# =============================================================================
section("STEP 15 — Generating Markdown audit report")

now = datetime.now().strftime("%Y-%m-%d %H:%M:%S")


def null_summary_table(raw: dict, after: dict, cols: list) -> str:
    rows = [
        [col, raw.get(col, 0), after.get(col, 0), raw.get(col, 0) - after.get(col, 0)]
        for col in cols
    ]
    return md_table(["Column", "NaN (raw)", "NaN (pre-Step-13)", "Reduced by"], rows)


step13_rows = [
    [e["dataset"], e["column"], f"`{e['action']}`", e["n"]]
    for e in _LOG if e["step"] == "Step 13"
]

step13_table = (
    md_table(["Dataset", "Column", "Strategy", "NaN Count"], step13_rows)
    if step13_rows
    else "_No remaining NaN after imputation — no fallbacks applied._"
)

step10_data = [e for e in _LOG if e["step"].startswith("Step 10")]
step11_data = [
    [e["dataset"], e["column"],
     f"{cap_log.get(e['dataset'], {}).get(e['column'], '—'):.4f}"
     if isinstance(cap_log.get(e["dataset"], {}).get(e["column"]), float) else "—",
     e["n"]]
    for e in _LOG if e["step"] == "Step 11"
]
step9_rows = [
    [ds, d["total"], d["rescued"], d["nulled"]]
    for ds, d in step9_details.items()
    if d["total"] > 0
]

md = f"""# Equipment Failure Claims — Data Cleaning Report

**Generated:** {now}
**Script:** `clean_equipment_claims.py`

---

## 1. Overview

| | Frequency Dataset | Severity Dataset |
|---|---|---|
| Input file | `{FREQ_IN}` | `{SEV_IN}` |
| Output file | `{FREQ_OUT}` | `{SEV_OUT}` |
| Raw rows | {raw_rows_freq:,} | {raw_rows_sev:,} |
| Cleaned rows | {len(freq):,} | {len(sev):,} |
| Raw columns | {raw_cols_freq} | {raw_cols_sev} |
| Cleaned columns | {freq.shape[1]} | {sev.shape[1]} |

---

## 2. Pipeline Changes vs Previous Version

### Key corrections in this version

| # | What changed | Why |
|---|---|---|
| 1 | `policy_id` cleaning and imputation moved to Steps 3–5 (before everything else) | Downstream cross-imputation and claim_count derivation both rely on a clean policy_id key — running them first maximises coverage |
| 2 | `claim_count` now derived as `COUNT(sev rows per policy_id)` not `max(claim_seq)` | Row count is the correct definition per requirements; max(claim_seq) fails if seq has gaps, duplicates, or NaN |
| 3 | `claim_count` is re-synced after Step 11a drops claim_amount outlier rows | Keeps freq and sev consistent after the drop |

---

## 3. Issues Found & Actions Taken

### 3.1 Non-standard file format

Both CSVs use **semicolons** as delimiter and **commas** as decimal separator.
Loaded with `sep=';'` and `decimal=','`. Output uses `decimal='.'`.

---

### 3.2 policy_id imputation (Steps 3–5)

policy_id is recovered in three stages before any other cleaning occurs.

**Step 3 — Direct equipment_id match:**

{md_table(["Target", "Source", "Filled"],
    [[e["dataset"], e["action"].split("from ")[-1] if "from" in e["action"] else "—", e["n"]]
     for e in _LOG if e["step"] == "Step 3" and e["column"] == "policy_id"])
    if any(e["step"] == "Step 3" and e["column"] == "policy_id" for e in _LOG)
    else "_No values filled via direct match._"}

**Step 4 — Secondary key combinations:**

{md_table(["Target", "Key Used", "Filled"],
    [[e["dataset"], e["action"].replace("imputed via ",""), e["n"]]
     for e in _LOG if e["step"] == "Step 4"])
    if any(e["step"] == "Step 4" for e in _LOG)
    else "_No values filled via secondary keys._"}

**Step 5 — Placeholders assigned:**

{md_table(["Dataset", "Column", "Count", "Range"],
    [[e["dataset"], e["column"], e["n"],
      next((l.split("->")[-1].strip() for l in _RAW_LOG
            if e["column"] in l and e["dataset"] in l and "NaN ->" in l), "—")]
     for e in _LOG if e["step"] == "Step 5"])
    if any(e["step"] == "Step 5" for e in _LOG)
    else "_No placeholder IDs needed._"}

---

### 3.3 Corrupted categorical values (Step 6)

{md_table(["Dataset", "Column", "Values Cleared"],
    [[e["dataset"], e["column"], e["n"]] for e in _LOG if e["step"] == "Step 6"])}

---

### 3.4 Cross-imputation of shared columns (Step 7)

{md_table(["Target", "Column", "Values Filled"],
    [[e["dataset"], e["column"], e["n"]] for e in _LOG if e["step"] == "Step 7"])
    if any(e["step"] == "Step 7" for e in _LOG)
    else "_No cross-imputation possible._"}

---

### 3.5 Negative values (Step 8)

{md_table(["Dataset", "Column", "Count"],
    [[e["dataset"], e["column"], e["n"]] for e in _LOG if e["step"] == "Step 8"])
    if any(e["step"] == "Step 8" for e in _LOG)
    else "_No negatives found._"}

---

### 3.6 Out-of-range exposure (Step 9)

{md_table(["Dataset", "Values > 1", "Rescued", "Set to NaN"], step9_rows)
    if step9_rows else "_No out-of-range exposure values._"}

---

### 3.7 claim_count and claim_seq (Step 10)

`claim_count` is defined as the **count of rows in sev for that policy_id**.
Non-integer floats and negatives were nulled first, then every `claim_count`
value was derived (or verified) directly from `COUNT(sev rows per policy_id)`.

| Sub-step | Dataset | Column | Values Affected |
|---|---|---|---|
{chr(10).join(
    f"| {e['step']} | {e['dataset']} | {e['column']} | {e['n']} |"
    for e in step10_data
)}

---

### 3.8 Percentile capping (Step 11)

{md_table(["Dataset", "Column", "Cap Value", "Values Capped"], step11_data)
    if step11_data else "_No values exceeded the 99th percentile._"}

---

### 3.9 claim_amount — rows dropped above 99th percentile (Step 11a)

{f"""| Metric | Value |
|---|---|
| 99th percentile threshold | {_amt_cap:,.4f} |
| Rows dropped | {_amt_dropped_n:,} |
| Rows remaining in sev | {len(sev):,} |
| claim_count rows re-synced in freq | {n_resync:,} |"""
if _amt_cap is not None else "_Step skipped — no valid claim_amount values._"}

{"**Dropped claim_ids:** " + ", ".join(f"`{x}`" for x in _amt_dropped_ids)
 if _amt_dropped_ids else ""}

---

## 4. Missing Value Summary

### 4.1 Frequency dataset

{null_summary_table(raw_nulls_freq, final_nulls_before_freq, list(raw_nulls_freq.keys()))}

### 4.2 Severity dataset

{null_summary_table(raw_nulls_sev, final_nulls_before_sev, list(raw_nulls_sev.keys()))}

---

## 5. Fallback Strategies Applied (Step 13)

{step13_table}

---

## 6. How to Change Fallback Strategies

Edit `UNRESOLVABLE_STRATEGY` at the top of the script and re-run.

| Strategy | Effect |
|---|---|
| `flag` | Keeps NaN; adds `<col>_missing` boolean column |
| `drop` | Drops the entire row |
| `median` | Fills with column median (numeric only) |
| `mode` | Fills with most frequent value |
| `zero` | Fills with 0 (numeric only) |
| `unknown` | Fills with `"UNKNOWN"` (categorical only) |

---

## 7. Full Console Audit Trail

```
{chr(10).join(_RAW_LOG)}
```

---

*Report generated by `clean_equipment_claims.py`*
"""

Path(REPORT).write_text(md, encoding="utf-8")
record(f"  Saved -> {REPORT}")

section("ALL STEPS COMPLETE")