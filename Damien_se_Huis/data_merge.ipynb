{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ef316343",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Installing statsmodels...\n",
      "  ✓ statsmodels installed\n",
      "  Installing matplotlib...\n",
      "  ✓ matplotlib installed\n",
      "✓ All packages ready\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "\n",
    "def install_if_missing(packages: dict[str, str]):\n",
    "    \"\"\"\n",
    "    Auto-installs missing packages.\n",
    "    Key = import name, Value = pip install name (if different).\n",
    "    \"\"\"\n",
    "    for import_name, pip_name in packages.items():\n",
    "        try:\n",
    "            __import__(import_name)\n",
    "        except ImportError:\n",
    "            print(f\"  Installing {pip_name}...\")\n",
    "            subprocess.check_call(\n",
    "                [sys.executable, \"-m\", \"pip\", \"install\", pip_name, \"-q\"]\n",
    "            )\n",
    "            print(f\"  ✓ {pip_name} installed\")\n",
    "\n",
    "install_if_missing({\n",
    "    \"pyarrow\":      \"pyarrow\",\n",
    "    \"pandas\":       \"pandas\",\n",
    "    \"numpy\":        \"numpy\",\n",
    "    \"statsmodels\":  \"statsmodels\",\n",
    "    \"sklearn\":      \"scikit-learn\",\n",
    "    \"matplotlib\":   \"matplotlib\",\n",
    "})\n",
    "\n",
    "# ── Imports ───────────────────────────────────────────────────────────────────\n",
    "import pyarrow.parquet as pq\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import TweedieRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_tweedie_deviance\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "print(\"✓ All packages ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6ce8f783",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_freq = pq.read_table(\"clean_business_claims_freq.parquet\").to_pandas()\n",
    "df_sev = pq.read_table(\"clean_business_claims_sev.parquet\").to_pandas()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cb717fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_freq.sort_values(by=\"solar_system\", inplace=True)\n",
    "df_sev.sort_values(by=\"solar_system\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e49d03b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>policy_id</th>\n",
       "      <th>station_id</th>\n",
       "      <th>solar_system</th>\n",
       "      <th>production_load</th>\n",
       "      <th>exposure</th>\n",
       "      <th>energy_backup_score</th>\n",
       "      <th>safety_compliance</th>\n",
       "      <th>claim_amount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BI-000010</td>\n",
       "      <td>G3</td>\n",
       "      <td>Zeta</td>\n",
       "      <td>0.688</td>\n",
       "      <td>0.300</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>4118460.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BI-000015</td>\n",
       "      <td>G2</td>\n",
       "      <td>Helionis Cluster</td>\n",
       "      <td>0.414</td>\n",
       "      <td>0.570</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>961887.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>BI-000028</td>\n",
       "      <td>A9</td>\n",
       "      <td>Zeta</td>\n",
       "      <td>0.355</td>\n",
       "      <td>0.334</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>522526.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>BI-000056</td>\n",
       "      <td>B3</td>\n",
       "      <td>Epsilon</td>\n",
       "      <td>0.457</td>\n",
       "      <td>0.761</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>1152033.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>BI-000062</td>\n",
       "      <td>B2</td>\n",
       "      <td>Epsilon</td>\n",
       "      <td>0.890</td>\n",
       "      <td>0.821</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>2061466.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>BI-000072</td>\n",
       "      <td>B3</td>\n",
       "      <td>Epsilon</td>\n",
       "      <td>0.023</td>\n",
       "      <td>0.614</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>2860153.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>BI-000081</td>\n",
       "      <td>G1</td>\n",
       "      <td>Zeta</td>\n",
       "      <td>0.582</td>\n",
       "      <td>0.724</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>9022207.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>BI-000085</td>\n",
       "      <td>G3</td>\n",
       "      <td>Epsilon</td>\n",
       "      <td>0.671</td>\n",
       "      <td>0.634</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3094557.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>BI-000092</td>\n",
       "      <td>G3</td>\n",
       "      <td>Zeta</td>\n",
       "      <td>0.821</td>\n",
       "      <td>0.713</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>5026302.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>BI-000103</td>\n",
       "      <td>G5</td>\n",
       "      <td>Zeta</td>\n",
       "      <td>0.626</td>\n",
       "      <td>0.628</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>2269630.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   policy_id station_id      solar_system  production_load  exposure  \\\n",
       "0  BI-000010         G3              Zeta            0.688     0.300   \n",
       "1  BI-000015         G2  Helionis Cluster            0.414     0.570   \n",
       "2  BI-000028         A9              Zeta            0.355     0.334   \n",
       "3  BI-000056         B3           Epsilon            0.457     0.761   \n",
       "4  BI-000062         B2           Epsilon            0.890     0.821   \n",
       "5  BI-000072         B3           Epsilon            0.023     0.614   \n",
       "6  BI-000081         G1              Zeta            0.582     0.724   \n",
       "7  BI-000085         G3           Epsilon            0.671     0.634   \n",
       "8  BI-000092         G3              Zeta            0.821     0.713   \n",
       "9  BI-000103         G5              Zeta            0.626     0.628   \n",
       "\n",
       "   energy_backup_score  safety_compliance  claim_amount  \n",
       "0                    4                  3     4118460.0  \n",
       "1                    3                  5      961887.0  \n",
       "2                    3                  1      522526.0  \n",
       "3                    4                  3     1152033.0  \n",
       "4                    4                  4     2061466.0  \n",
       "5                    2                  4     2860153.0  \n",
       "6                    5                  4     9022207.0  \n",
       "7                    1                  2     3094557.0  \n",
       "8                    5                  3     5026302.0  \n",
       "9                    4                  4     2269630.0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Grouping sev dataset based on policy id and summing claim_amount\n",
    "agg_dict = {col: 'first' for col in df_sev.columns if col not in ['claim_id', 'claim_seq', 'policy_id', 'claim_amount']}\n",
    "agg_dict['claim_amount'] = 'sum'\n",
    "\n",
    "df_grouped_business_claims_sev = (\n",
    "    df_sev.drop(columns=['claim_id', 'claim_seq'])\n",
    "    .groupby('policy_id', as_index=False)\n",
    "    .agg(agg_dict)\n",
    ")\n",
    "\n",
    "df_grouped_business_claims_sev.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1ea51820",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zero claims: 92.9%\n"
     ]
    }
   ],
   "source": [
    "#merging the two datasets on policy_id\n",
    "sev_claim_amounts = df_sev.groupby('policy_id', as_index=False)['claim_amount'].sum()\n",
    "\n",
    "df_combined = df_freq.merge(\n",
    "    sev_claim_amounts, \n",
    "    on='policy_id', \n",
    "    how='left'\n",
    ")\n",
    "\n",
    "df_combined['claim_amount'] = df_combined['claim_amount'].fillna(0)\n",
    "\n",
    "df_combined.columns\n",
    "zero_ratio = (df_combined['claim_amount'] == 0).mean()\n",
    "print(f\"Zero claims: {zero_ratio:.1%}\")\n",
    "\n",
    "#df_combined.to_csv(\"bussiness_clams_model_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "df4cd73d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "── Step 1: Preparing data ──────────────────────────────────────────\n",
      "  Frequency rows : 99,923\n",
      "  Severity rows  : 7,055  (92.9% zero-claim policies excluded)\n",
      "\n",
      "── Step 2: Train / Test split ──────────────────────────────────────\n",
      "  Train freq: 79,938  |  Train sev: 5,644\n",
      "  Test  freq: 19,985  |  Test  sev: 1,411\n",
      "\n",
      "── Step 3: Frequency Model (Poisson GLM) ───────────────────────────\n",
      "── Frequency: Backward AIC selection ──────────────────────────────\n",
      "    Dropping 'station_id_A6' → AIC inf → 57923.8\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 329\u001b[39m\n\u001b[32m    316\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[32m    317\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mfreq_model\u001b[39m\u001b[33m\"\u001b[39m:    freq_model,\n\u001b[32m    318\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33msev_model\u001b[39m\u001b[33m\"\u001b[39m:     sev_model,\n\u001b[32m   (...)\u001b[39m\u001b[32m    324\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mtest_metrics\u001b[39m\u001b[33m\"\u001b[39m:  test_metrics,\n\u001b[32m    325\u001b[39m     }\n\u001b[32m    328\u001b[39m \u001b[38;5;66;03m# ── Run ───────────────────────────────────────────────────────────────────────\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m329\u001b[39m results = \u001b[43mrun_freq_sev_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_combined\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 292\u001b[39m, in \u001b[36mrun_freq_sev_model\u001b[39m\u001b[34m(df)\u001b[39m\n\u001b[32m    290\u001b[39m \u001b[38;5;66;03m# ── Step 3: Frequency model\u001b[39;00m\n\u001b[32m    291\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m── Step 3: Frequency Model (Poisson GLM) ───────────────────────────\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m292\u001b[39m freq_model, freq_selected, freq_enc_cols = \u001b[43mfit_frequency_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextended_features\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    293\u001b[39m \u001b[38;5;28mprint\u001b[39m(freq_model.summary())\n\u001b[32m    295\u001b[39m \u001b[38;5;66;03m# ── Step 4: Severity model\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 133\u001b[39m, in \u001b[36mfit_frequency_model\u001b[39m\u001b[34m(train_df, feature_cols)\u001b[39m\n\u001b[32m    127\u001b[39m df_enc = pd.get_dummies(\n\u001b[32m    128\u001b[39m     train_df[feature_cols + [TARGET_FREQ, \u001b[33m\"\u001b[39m\u001b[33mlog_exposure\u001b[39m\u001b[33m\"\u001b[39m]],\n\u001b[32m    129\u001b[39m     columns=CATEGORICAL_FEATURES, drop_first=\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    130\u001b[39m )\n\u001b[32m    131\u001b[39m enc_feature_cols = [c \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m df_enc.columns \u001b[38;5;28;01mif\u001b[39;00m c \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m [TARGET_FREQ, \u001b[33m\"\u001b[39m\u001b[33mlog_exposure\u001b[39m\u001b[33m\"\u001b[39m]]\n\u001b[32m--> \u001b[39m\u001b[32m133\u001b[39m selected = \u001b[43mbackward_aic\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    134\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdf_enc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menc_feature_cols\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mTARGET_FREQ\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    135\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfamily\u001b[49m\u001b[43m=\u001b[49m\u001b[43msm\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfamilies\u001b[49m\u001b[43m.\u001b[49m\u001b[43mPoisson\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlink\u001b[49m\u001b[43m=\u001b[49m\u001b[43msm\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfamilies\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlinks\u001b[49m\u001b[43m.\u001b[49m\u001b[43mLog\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    136\u001b[39m \u001b[43m    \u001b[49m\u001b[43moffset_col\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlog_exposure\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    137\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    139\u001b[39m X = sm.add_constant(df_enc[selected].astype(\u001b[38;5;28mfloat\u001b[39m))\n\u001b[32m    140\u001b[39m model = sm.GLM(\n\u001b[32m    141\u001b[39m     df_enc[TARGET_FREQ].astype(\u001b[38;5;28mfloat\u001b[39m), X,\n\u001b[32m    142\u001b[39m     family=sm.families.Poisson(link=sm.families.links.Log()),\n\u001b[32m    143\u001b[39m     offset=df_enc[\u001b[33m\"\u001b[39m\u001b[33mlog_exposure\u001b[39m\u001b[33m\"\u001b[39m].astype(\u001b[38;5;28mfloat\u001b[39m),\n\u001b[32m    144\u001b[39m ).fit(disp=\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 83\u001b[39m, in \u001b[36mbackward_aic\u001b[39m\u001b[34m(df, feature_cols, target, family, offset_col, weight_col)\u001b[39m\n\u001b[32m     80\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mfreq_weights\u001b[39m\u001b[33m\"\u001b[39m] = df[weight_col].astype(\u001b[38;5;28mfloat\u001b[39m)\n\u001b[32m     82\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m83\u001b[39m     result = \u001b[43msm\u001b[49m\u001b[43m.\u001b[49m\u001b[43mGLM\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfamily\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfamily\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdisp\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     84\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m result.aic < best_aic:\n\u001b[32m     85\u001b[39m         best_aic  = result.aic\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hendr\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\statsmodels\\genmod\\generalized_linear_model.py:1192\u001b[39m, in \u001b[36mGLM.fit\u001b[39m\u001b[34m(self, start_params, maxiter, method, tol, scale, cov_type, cov_kwds, use_t, full_output, disp, max_start_irls, **kwargs)\u001b[39m\n\u001b[32m   1190\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m cov_type.lower() == \u001b[33m'\u001b[39m\u001b[33meim\u001b[39m\u001b[33m'\u001b[39m:\n\u001b[32m   1191\u001b[39m         cov_type = \u001b[33m'\u001b[39m\u001b[33mnonrobust\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1192\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fit_irls\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstart_params\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstart_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmaxiter\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaxiter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1193\u001b[39m \u001b[43m                          \u001b[49m\u001b[43mtol\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtol\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale\u001b[49m\u001b[43m=\u001b[49m\u001b[43mscale\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcov_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcov_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1194\u001b[39m \u001b[43m                          \u001b[49m\u001b[43mcov_kwds\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcov_kwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_t\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_t\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1195\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1196\u001b[39m     \u001b[38;5;28mself\u001b[39m._optim_hessian = kwargs.get(\u001b[33m'\u001b[39m\u001b[33moptim_hessian\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hendr\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\statsmodels\\genmod\\generalized_linear_model.py:1333\u001b[39m, in \u001b[36mGLM._fit_irls\u001b[39m\u001b[34m(self, start_params, maxiter, tol, scale, cov_type, cov_kwds, use_t, **kwargs)\u001b[39m\n\u001b[32m   1328\u001b[39m wlsendog = (lin_pred + \u001b[38;5;28mself\u001b[39m.family.link.deriv(mu) * (\u001b[38;5;28mself\u001b[39m.endog-mu)\n\u001b[32m   1329\u001b[39m             - \u001b[38;5;28mself\u001b[39m._offset_exposure)\n\u001b[32m   1330\u001b[39m wls_mod = reg_tools._MinimalWLS(wlsendog, wlsexog,\n\u001b[32m   1331\u001b[39m                                 \u001b[38;5;28mself\u001b[39m.weights, check_endog=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m   1332\u001b[39m                                 check_weights=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m-> \u001b[39m\u001b[32m1333\u001b[39m wls_results = \u001b[43mwls_mod\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mwls_method\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1334\u001b[39m lin_pred = np.dot(\u001b[38;5;28mself\u001b[39m.exog, wls_results.params)\n\u001b[32m   1335\u001b[39m lin_pred += \u001b[38;5;28mself\u001b[39m._offset_exposure\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hendr\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\statsmodels\\regression\\_tools.py:101\u001b[39m, in \u001b[36m_MinimalWLS.fit\u001b[39m\u001b[34m(self, method)\u001b[39m\n\u001b[32m     99\u001b[39m     params = np.linalg.solve(R, np.dot(Q.T, \u001b[38;5;28mself\u001b[39m.wendog))\n\u001b[32m    100\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m101\u001b[39m     params, _, _, _ = \u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlinalg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlstsq\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mwexog\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mwendog\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    102\u001b[39m \u001b[43m                                      \u001b[49m\u001b[43mrcond\u001b[49m\u001b[43m=\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    103\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.results(params)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hendr\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\numpy\\linalg\\_linalg.py:2539\u001b[39m, in \u001b[36mlstsq\u001b[39m\u001b[34m(a, b, rcond)\u001b[39m\n\u001b[32m   2535\u001b[39m     b = zeros(b.shape[:-\u001b[32m2\u001b[39m] + (m, n_rhs + \u001b[32m1\u001b[39m), dtype=b.dtype)\n\u001b[32m   2537\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m errstate(call=_raise_linalgerror_lstsq, invalid=\u001b[33m'\u001b[39m\u001b[33mcall\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m   2538\u001b[39m               over=\u001b[33m'\u001b[39m\u001b[33mignore\u001b[39m\u001b[33m'\u001b[39m, divide=\u001b[33m'\u001b[39m\u001b[33mignore\u001b[39m\u001b[33m'\u001b[39m, under=\u001b[33m'\u001b[39m\u001b[33mignore\u001b[39m\u001b[33m'\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m2539\u001b[39m     x, resids, rank, s = \u001b[43m_umath_linalg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlstsq\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrcond\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2540\u001b[39m \u001b[43m                                             \u001b[49m\u001b[43msignature\u001b[49m\u001b[43m=\u001b[49m\u001b[43msignature\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2541\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m m == \u001b[32m0\u001b[39m:\n\u001b[32m   2542\u001b[39m     x[...] = \u001b[32m0\u001b[39m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "\n",
    "# ── 0. Imports ────────────────────────────────────────────────────────────────\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import PoissonRegressor, GammaRegressor, TweedieRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_tweedie_deviance\n",
    "\n",
    "\n",
    "# ── 1. Schema ─────────────────────────────────────────────────────────────────\n",
    "NUMERIC_FEATURES = [\n",
    "    \"production_load\", \"energy_backup_score\", \"supply_chain_index\",\n",
    "    \"avg_crew_exp\", \"maintenance_freq\", \"safety_compliance\",\n",
    "]\n",
    "CATEGORICAL_FEATURES = [\"station_id\", \"solar_system\"]\n",
    "ALL_FEATURES  = NUMERIC_FEATURES + CATEGORICAL_FEATURES\n",
    "TARGET_FREQ   = \"claim_count\"\n",
    "TARGET_SEV    = \"claim_amount\"\n",
    "EXPOSURE_COL  = \"exposure\"\n",
    "\n",
    "\n",
    "# ── 2. Data Prep ──────────────────────────────────────────────────────────────\n",
    "def prepare_data(df: pd.DataFrame) -> tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    df = df.copy()\n",
    "    df = df[df[EXPOSURE_COL] > 0]\n",
    "    df = df[df[TARGET_SEV] >= 0]\n",
    "    df[\"log_exposure\"] = np.log(df[EXPOSURE_COL])\n",
    "\n",
    "    # Frequency dataset — all rows\n",
    "    freq_df = df.copy()\n",
    "\n",
    "    # Severity dataset — only rows WHERE a claim occurred\n",
    "    sev_df = df[df[TARGET_SEV] > 0].copy()\n",
    "    # Severity target = average cost per claim\n",
    "    sev_df[\"avg_claim_amount\"] = sev_df[TARGET_SEV] / sev_df[TARGET_FREQ].clip(lower=1)\n",
    "\n",
    "    return freq_df, sev_df\n",
    "\n",
    "\n",
    "# ── 3. Preprocessor ───────────────────────────────────────────────────────────\n",
    "def build_preprocessor() -> ColumnTransformer:\n",
    "    return ColumnTransformer([\n",
    "        (\"num\", StandardScaler(), NUMERIC_FEATURES),\n",
    "        (\"cat\", OneHotEncoder(drop=\"first\", handle_unknown=\"ignore\"), CATEGORICAL_FEATURES),\n",
    "    ])\n",
    "\n",
    "\n",
    "# ── 4. Stepwise AIC Variable Selection (statsmodels) ─────────────────────────\n",
    "def backward_aic(\n",
    "    df: pd.DataFrame,\n",
    "    feature_cols: list[str],\n",
    "    target: str,\n",
    "    family,\n",
    "    offset_col: str = \"log_exposure\",\n",
    "    weight_col: str = None,\n",
    ") -> list[str]:\n",
    "    \"\"\"Backward stepwise selection — drops variables that reduce AIC.\"\"\"\n",
    "\n",
    "    remaining = feature_cols.copy()\n",
    "    current_aic = np.inf\n",
    "\n",
    "    while True:\n",
    "        best_aic   = current_aic\n",
    "        worst_var  = None\n",
    "\n",
    "        for var in remaining:\n",
    "            candidate = [v for v in remaining if v != var]\n",
    "            X = sm.add_constant(df[candidate].astype(float))\n",
    "            y = df[target].astype(float)\n",
    "\n",
    "            kwargs = {\"offset\": df[offset_col].astype(float)} if offset_col else {}\n",
    "            if weight_col:\n",
    "                kwargs[\"freq_weights\"] = df[weight_col].astype(float)\n",
    "\n",
    "            try:\n",
    "                result = sm.GLM(y, X, family=family, **kwargs).fit(disp=False)\n",
    "                if result.aic < best_aic:\n",
    "                    best_aic  = result.aic\n",
    "                    worst_var = var\n",
    "            except Exception:\n",
    "                continue\n",
    "\n",
    "        if worst_var is None or best_aic >= current_aic:\n",
    "            break\n",
    "\n",
    "        print(f\"    Dropping '{worst_var}' → AIC {current_aic:.1f} → {best_aic:.1f}\")\n",
    "        remaining.remove(worst_var)\n",
    "        current_aic = best_aic\n",
    "\n",
    "    print(f\"    ✓ Selected: {remaining}\\n\")\n",
    "    return remaining\n",
    "\n",
    "\n",
    "# ── 5. Interaction Features ───────────────────────────────────────────────────\n",
    "def add_interactions(df: pd.DataFrame) -> tuple[pd.DataFrame, list[str]]:\n",
    "    \"\"\"Add theoretically motivated interaction terms.\"\"\"\n",
    "    df = df.copy()\n",
    "    interactions = {\n",
    "        \"energy_x_safety\":    (\"energy_backup_score\", \"safety_compliance\"),\n",
    "        \"load_x_maintenance\": (\"production_load\",     \"maintenance_freq\"),\n",
    "        \"crew_x_supply\":      (\"avg_crew_exp\",        \"supply_chain_index\"),\n",
    "    }\n",
    "    new_cols = []\n",
    "    for name, (a, b) in interactions.items():\n",
    "        df[name] = df[a] * df[b]\n",
    "        new_cols.append(name)\n",
    "    return df, new_cols\n",
    "\n",
    "\n",
    "# ── 6. Frequency Model (Poisson GLM) ─────────────────────────────────────────\n",
    "def fit_frequency_model(\n",
    "    train_df: pd.DataFrame,\n",
    "    feature_cols: list[str],\n",
    ") -> tuple:\n",
    "    \"\"\"\n",
    "    Poisson GLM: E[claim_count] = exp(Xβ + log(exposure))\n",
    "    Returns (sm_result, selected_features)\n",
    "    \"\"\"\n",
    "    print(\"── Frequency: Backward AIC selection ──────────────────────────────\")\n",
    "    df_enc = pd.get_dummies(\n",
    "        train_df[feature_cols + [TARGET_FREQ, \"log_exposure\"]],\n",
    "        columns=CATEGORICAL_FEATURES, drop_first=True\n",
    "    )\n",
    "    enc_feature_cols = [c for c in df_enc.columns if c not in [TARGET_FREQ, \"log_exposure\"]]\n",
    "\n",
    "    selected = backward_aic(\n",
    "        df_enc, enc_feature_cols, TARGET_FREQ,\n",
    "        family=sm.families.Poisson(link=sm.families.links.Log()),\n",
    "        offset_col=\"log_exposure\",\n",
    "    )\n",
    "\n",
    "    X = sm.add_constant(df_enc[selected].astype(float))\n",
    "    model = sm.GLM(\n",
    "        df_enc[TARGET_FREQ].astype(float), X,\n",
    "        family=sm.families.Poisson(link=sm.families.links.Log()),\n",
    "        offset=df_enc[\"log_exposure\"].astype(float),\n",
    "    ).fit(disp=False)\n",
    "\n",
    "    return model, selected, df_enc.columns.tolist()\n",
    "\n",
    "\n",
    "# ── 7. Severity Model (Gamma GLM) ─────────────────────────────────────────────\n",
    "def fit_severity_model(\n",
    "    sev_train: pd.DataFrame,\n",
    "    feature_cols: list[str],\n",
    ") -> tuple:\n",
    "    \"\"\"\n",
    "    Gamma GLM: E[avg_claim_amount | claim > 0] = exp(Xβ)\n",
    "    Weighted by claim_count (more claims = more reliable severity estimate).\n",
    "    \"\"\"\n",
    "    print(\"── Severity: Backward AIC selection ───────────────────────────────\")\n",
    "    df_enc = pd.get_dummies(\n",
    "        sev_train[feature_cols + [\"avg_claim_amount\", TARGET_FREQ, \"log_exposure\"]],\n",
    "        columns=CATEGORICAL_FEATURES, drop_first=True\n",
    "    )\n",
    "    enc_feature_cols = [c for c in df_enc.columns\n",
    "                        if c not in [\"avg_claim_amount\", TARGET_FREQ, \"log_exposure\"]]\n",
    "\n",
    "    selected = backward_aic(\n",
    "        df_enc, enc_feature_cols, \"avg_claim_amount\",\n",
    "        family=sm.families.Gamma(link=sm.families.links.Log()),\n",
    "        offset_col=None,  # no offset for severity\n",
    "        weight_col=TARGET_FREQ,\n",
    "    )\n",
    "\n",
    "    X = sm.add_constant(df_enc[selected].astype(float))\n",
    "    model = sm.GLM(\n",
    "        df_enc[\"avg_claim_amount\"].astype(float), X,\n",
    "        family=sm.families.Gamma(link=sm.families.links.Log()),\n",
    "        freq_weights=df_enc[TARGET_FREQ].astype(float),\n",
    "    ).fit(disp=False)\n",
    "\n",
    "    return model, selected, df_enc.columns.tolist()\n",
    "\n",
    "\n",
    "# ── 8. Prediction ─────────────────────────────────────────────────────────────\n",
    "def predict_pure_premium(\n",
    "    df: pd.DataFrame,\n",
    "    freq_model,\n",
    "    sev_model,\n",
    "    freq_selected: list[str],\n",
    "    sev_selected: list[str],\n",
    "    all_enc_cols_freq: list[str],\n",
    "    all_enc_cols_sev: list[str],\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Pure Premium = E[frequency] × E[severity]\n",
    "                 = (predicted claims per unit exposure) × (predicted cost per claim)\n",
    "    \"\"\"\n",
    "    def _encode(df_, cat_cols, all_enc_cols):\n",
    "        df_enc = pd.get_dummies(df_, columns=cat_cols, drop_first=True)\n",
    "        # Align columns to training schema\n",
    "        for col in all_enc_cols:\n",
    "            if col not in df_enc.columns:\n",
    "                df_enc[col] = 0\n",
    "        return df_enc\n",
    "\n",
    "    # Frequency prediction\n",
    "    freq_enc = _encode(df[ALL_FEATURES + [\"log_exposure\"]], CATEGORICAL_FEATURES, all_enc_cols_freq)\n",
    "    X_freq   = sm.add_constant(freq_enc[freq_selected].astype(float), has_constant=\"add\")\n",
    "    pred_freq = freq_model.predict(X_freq, offset=freq_enc[\"log_exposure\"].astype(float))\n",
    "\n",
    "    # Severity prediction\n",
    "    sev_enc  = _encode(df[ALL_FEATURES], CATEGORICAL_FEATURES, all_enc_cols_sev)\n",
    "    X_sev    = sm.add_constant(sev_enc[sev_selected].astype(float), has_constant=\"add\")\n",
    "    pred_sev = sev_model.predict(X_sev)\n",
    "\n",
    "    # Pure premium = freq rate × severity (already per-unit-exposure from Poisson offset)\n",
    "    pure_premium = pred_freq * pred_sev\n",
    "    return pure_premium\n",
    "\n",
    "\n",
    "# ── 9. Evaluation ─────────────────────────────────────────────────────────────\n",
    "def evaluate(df: pd.DataFrame, pred_pp: np.ndarray, label: str = \"\"):\n",
    "    actual_pp = df[TARGET_SEV] / df[EXPOSURE_COL]\n",
    "    mae       = mean_absolute_error(actual_pp, pred_pp)\n",
    "    corr      = np.corrcoef(actual_pp, pred_pp)[0, 1]\n",
    "\n",
    "    # Lift chart — are high-risk policies ranked correctly?\n",
    "    n_deciles    = 10\n",
    "    df_eval      = pd.DataFrame({\"actual\": actual_pp, \"predicted\": pred_pp})\n",
    "    df_eval[\"decile\"] = pd.qcut(df_eval[\"predicted\"], n_deciles, labels=False, duplicates=\"drop\")\n",
    "    lift         = df_eval.groupby(\"decile\")[[\"actual\", \"predicted\"]].mean()\n",
    "\n",
    "    print(f\"\\n{'═'*45}\")\n",
    "    print(f\"  {label}\")\n",
    "    print(f\"  MAE (pure premium):    {mae:>15,.2f}\")\n",
    "    print(f\"  Pearson correlation:   {corr:>15.4f}\")\n",
    "    print(f\"{'═'*45}\")\n",
    "\n",
    "    # Plot\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(13, 5))\n",
    "\n",
    "    axes[0].scatter(actual_pp, pred_pp, alpha=0.2, s=8)\n",
    "    lim = max(actual_pp.max(), pred_pp.max())\n",
    "    axes[0].plot([0, lim], [0, lim], \"r--\")\n",
    "    axes[0].set_xlabel(\"Actual Pure Premium\")\n",
    "    axes[0].set_ylabel(\"Predicted Pure Premium\")\n",
    "    axes[0].set_title(f\"Actual vs Predicted — {label}\")\n",
    "\n",
    "    axes[1].plot(lift.index, lift[\"actual\"],    \"o-\", label=\"Actual\")\n",
    "    axes[1].plot(lift.index, lift[\"predicted\"], \"s--\", label=\"Predicted\")\n",
    "    axes[1].set_xlabel(\"Risk Decile (predicted)\")\n",
    "    axes[1].set_ylabel(\"Mean Pure Premium\")\n",
    "    axes[1].set_title(\"Lift Chart by Risk Decile\")\n",
    "    axes[1].legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return {\"mae\": mae, \"correlation\": corr, \"lift\": lift}\n",
    "\n",
    "\n",
    "# ── 10. Main ──────────────────────────────────────────────────────────────────\n",
    "def run_freq_sev_model(df: pd.DataFrame):\n",
    "\n",
    "    # ── Step 1: Prep\n",
    "    print(\"── Step 1: Preparing data ──────────────────────────────────────────\")\n",
    "    freq_df, sev_df = prepare_data(df)\n",
    "\n",
    "    freq_df, interaction_cols = add_interactions(freq_df)\n",
    "    sev_df,  _               = add_interactions(sev_df)\n",
    "    extended_features = ALL_FEATURES + interaction_cols\n",
    "\n",
    "    zero_pct = (freq_df[TARGET_SEV] == 0).mean()\n",
    "    print(f\"  Frequency rows : {len(freq_df):,}\")\n",
    "    print(f\"  Severity rows  : {len(sev_df):,}  ({zero_pct:.1%} zero-claim policies excluded)\")\n",
    "\n",
    "    # ── Step 2: Split\n",
    "    print(\"\\n── Step 2: Train / Test split ──────────────────────────────────────\")\n",
    "    freq_df[\"_zero\"] = (freq_df[TARGET_SEV] == 0).astype(int)\n",
    "    train_freq, test_freq = train_test_split(\n",
    "        freq_df, test_size=0.2, random_state=42, stratify=freq_df[\"_zero\"]\n",
    "    )\n",
    "    train_freq = train_freq.drop(columns=\"_zero\")\n",
    "    test_freq  = test_freq.drop(columns=\"_zero\")\n",
    "\n",
    "    train_sev  = sev_df[sev_df.index.isin(train_freq.index)]\n",
    "    test_sev   = sev_df[sev_df.index.isin(test_freq.index)]\n",
    "    print(f\"  Train freq: {len(train_freq):,}  |  Train sev: {len(train_sev):,}\")\n",
    "    print(f\"  Test  freq: {len(test_freq):,}  |  Test  sev: {len(test_sev):,}\")\n",
    "\n",
    "    # ── Step 3: Frequency model\n",
    "    print(\"\\n── Step 3: Frequency Model (Poisson GLM) ───────────────────────────\")\n",
    "    freq_model, freq_selected, freq_enc_cols = fit_frequency_model(train_freq, extended_features)\n",
    "    print(freq_model.summary())\n",
    "\n",
    "    # ── Step 4: Severity model\n",
    "    print(\"\\n── Step 4: Severity Model (Gamma GLM) ──────────────────────────────\")\n",
    "    sev_model, sev_selected, sev_enc_cols = fit_severity_model(train_sev, extended_features)\n",
    "    print(sev_model.summary())\n",
    "\n",
    "    # ── Step 5: Predict pure premium\n",
    "    print(\"\\n── Step 5: Pure Premium = Frequency × Severity ─────────────────────\")\n",
    "    train_pp = predict_pure_premium(\n",
    "        train_freq, freq_model, sev_model,\n",
    "        freq_selected, sev_selected, freq_enc_cols, sev_enc_cols\n",
    "    )\n",
    "    test_pp  = predict_pure_premium(\n",
    "        test_freq, freq_model, sev_model,\n",
    "        freq_selected, sev_selected, freq_enc_cols, sev_enc_cols\n",
    "    )\n",
    "\n",
    "    # ── Step 6: Evaluate\n",
    "    print(\"\\n── Step 6: Evaluation ──────────────────────────────────────────────\")\n",
    "    train_metrics = evaluate(train_freq, train_pp, label=\"Train\")\n",
    "    test_metrics  = evaluate(test_freq,  test_pp,  label=\"Test\")\n",
    "\n",
    "    return {\n",
    "        \"freq_model\":    freq_model,\n",
    "        \"sev_model\":     sev_model,\n",
    "        \"freq_selected\": freq_selected,\n",
    "        \"sev_selected\":  sev_selected,\n",
    "        \"freq_enc_cols\": freq_enc_cols,\n",
    "        \"sev_enc_cols\":  sev_enc_cols,\n",
    "        \"train_metrics\": train_metrics,\n",
    "        \"test_metrics\":  test_metrics,\n",
    "    }\n",
    "\n",
    "\n",
    "# ── Run ───────────────────────────────────────────────────────────────────────\n",
    "results = run_freq_sev_model(df_combined)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
