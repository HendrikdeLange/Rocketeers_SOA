{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "912c5412",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\hendr\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (3.0.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\hendr\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (2.4.2)\n",
      "Requirement already satisfied: pyarrow in c:\\users\\hendr\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (23.0.1)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\hendr\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (1.8.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\hendr\\appdata\\roaming\\python\\python313\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: tzdata in c:\\users\\hendr\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pandas) (2025.3)\n",
      "Requirement already satisfied: scipy>=1.10.0 in c:\\users\\hendr\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from scikit-learn) (1.17.1)\n",
      "Requirement already satisfied: joblib>=1.3.0 in c:\\users\\hendr\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from scikit-learn) (1.5.3)\n",
      "Requirement already satisfied: threadpoolctl>=3.2.0 in c:\\users\\hendr\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\hendr\\appdata\\roaming\\python\\python313\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 26.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import sys\n",
    "!{sys.executable} -m pip install pandas numpy pyarrow scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ef316343",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyarrow.parquet as pq\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import TweedieRegressor\n",
    "from sklearn.metrics import d2_tweedie_score, mean_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6ce8f783",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_freq = pq.read_table(\"clean_business_claims_freq.parquet\").to_pandas()\n",
    "df_sev = pq.read_table(\"clean_business_claims_sev.parquet\").to_pandas()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cb717fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_freq.sort_values(by=\"solar_system\", inplace=True)\n",
    "df_sev.sort_values(by=\"solar_system\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e49d03b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>policy_id</th>\n",
       "      <th>station_id</th>\n",
       "      <th>solar_system</th>\n",
       "      <th>production_load</th>\n",
       "      <th>exposure</th>\n",
       "      <th>energy_backup_score</th>\n",
       "      <th>safety_compliance</th>\n",
       "      <th>claim_amount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BI-000010</td>\n",
       "      <td>G3</td>\n",
       "      <td>Zeta</td>\n",
       "      <td>0.688</td>\n",
       "      <td>0.300</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>4118460.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BI-000015</td>\n",
       "      <td>G2</td>\n",
       "      <td>Helionis Cluster</td>\n",
       "      <td>0.414</td>\n",
       "      <td>0.570</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>961887.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>BI-000028</td>\n",
       "      <td>A9</td>\n",
       "      <td>Zeta</td>\n",
       "      <td>0.355</td>\n",
       "      <td>0.334</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>522526.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>BI-000056</td>\n",
       "      <td>B3</td>\n",
       "      <td>Epsilon</td>\n",
       "      <td>0.457</td>\n",
       "      <td>0.761</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>1152033.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>BI-000062</td>\n",
       "      <td>B2</td>\n",
       "      <td>Epsilon</td>\n",
       "      <td>0.890</td>\n",
       "      <td>0.821</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>2061466.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>BI-000072</td>\n",
       "      <td>B3</td>\n",
       "      <td>Epsilon</td>\n",
       "      <td>0.023</td>\n",
       "      <td>0.614</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>2860153.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>BI-000081</td>\n",
       "      <td>G1</td>\n",
       "      <td>Zeta</td>\n",
       "      <td>0.582</td>\n",
       "      <td>0.724</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>9022207.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>BI-000085</td>\n",
       "      <td>G3</td>\n",
       "      <td>Epsilon</td>\n",
       "      <td>0.671</td>\n",
       "      <td>0.634</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3094557.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>BI-000092</td>\n",
       "      <td>G3</td>\n",
       "      <td>Zeta</td>\n",
       "      <td>0.821</td>\n",
       "      <td>0.713</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>5026302.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>BI-000103</td>\n",
       "      <td>G5</td>\n",
       "      <td>Zeta</td>\n",
       "      <td>0.626</td>\n",
       "      <td>0.628</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>2269630.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   policy_id station_id      solar_system  production_load  exposure  \\\n",
       "0  BI-000010         G3              Zeta            0.688     0.300   \n",
       "1  BI-000015         G2  Helionis Cluster            0.414     0.570   \n",
       "2  BI-000028         A9              Zeta            0.355     0.334   \n",
       "3  BI-000056         B3           Epsilon            0.457     0.761   \n",
       "4  BI-000062         B2           Epsilon            0.890     0.821   \n",
       "5  BI-000072         B3           Epsilon            0.023     0.614   \n",
       "6  BI-000081         G1              Zeta            0.582     0.724   \n",
       "7  BI-000085         G3           Epsilon            0.671     0.634   \n",
       "8  BI-000092         G3              Zeta            0.821     0.713   \n",
       "9  BI-000103         G5              Zeta            0.626     0.628   \n",
       "\n",
       "   energy_backup_score  safety_compliance  claim_amount  \n",
       "0                    4                  3     4118460.0  \n",
       "1                    3                  5      961887.0  \n",
       "2                    3                  1      522526.0  \n",
       "3                    4                  3     1152033.0  \n",
       "4                    4                  4     2061466.0  \n",
       "5                    2                  4     2860153.0  \n",
       "6                    5                  4     9022207.0  \n",
       "7                    1                  2     3094557.0  \n",
       "8                    5                  3     5026302.0  \n",
       "9                    4                  4     2269630.0  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Grouping sev dataset based on policy id and summing claim_amount\n",
    "agg_dict = {col: 'first' for col in df_sev.columns if col not in ['claim_id', 'claim_seq', 'policy_id', 'claim_amount']}\n",
    "agg_dict['claim_amount'] = 'sum'\n",
    "\n",
    "df_grouped_business_claims_sev = (\n",
    "    df_sev.drop(columns=['claim_id', 'claim_seq'])\n",
    "    .groupby('policy_id', as_index=False)\n",
    "    .agg(agg_dict)\n",
    ")\n",
    "\n",
    "df_grouped_business_claims_sev.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1ea51820",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['policy_id', 'station_id', 'solar_system', 'production_load',\n",
       "       'energy_backup_score', 'supply_chain_index', 'avg_crew_exp',\n",
       "       'maintenance_freq', 'safety_compliance', 'exposure', 'claim_count',\n",
       "       'claim_amount'],\n",
       "      dtype='str')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#merging the two datasets on policy_id\n",
    "sev_claim_amounts = df_sev.groupby('policy_id', as_index=False)['claim_amount'].sum()\n",
    "\n",
    "df_combined = df_freq.merge(\n",
    "    sev_claim_amounts, \n",
    "    on='policy_id', \n",
    "    how='left'\n",
    ")\n",
    "\n",
    "df_combined['claim_amount'] = df_combined['claim_amount'].fillna(0)\n",
    "\n",
    "df_combined.columns\n",
    "\n",
    "#df_combined.to_csv(\"bussiness_clams_model_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df4cd73d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── 0. Imports ────────────────────────────────────────────────────────────────\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import TweedieRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_tweedie_deviance\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "# ── 1. Feature Schema ─────────────────────────────────────────────────────────\n",
    "NUMERIC_FEATURES = [\n",
    "    \"production_load\",\n",
    "    \"energy_backup_score\",\n",
    "    \"supply_chain_index\",\n",
    "    \"avg_crew_exp\",\n",
    "    \"maintenance_freq\",\n",
    "    \"safety_compliance\",\n",
    "]\n",
    "\n",
    "CATEGORICAL_FEATURES = [\n",
    "    \"station_id\",\n",
    "    \"solar_system\",\n",
    "]\n",
    "\n",
    "TARGET          = \"claim_amount\"\n",
    "EXPOSURE_COL    = \"exposure\"\n",
    "\n",
    "\n",
    "# ── 2. Data Prep ──────────────────────────────────────────────────────────────\n",
    "def prepare_data(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "\n",
    "    # Safety: drop rows with non-positive exposure or negative claims\n",
    "    df = df[df[EXPOSURE_COL] > 0]\n",
    "    df = df[df[TARGET] >= 0]\n",
    "\n",
    "    # Pure premium (used for evaluation, not model input)\n",
    "    df[\"pure_premium\"] = df[TARGET] / df[EXPOSURE_COL]\n",
    "\n",
    "    # Log-offset for GLM\n",
    "    df[\"log_exposure\"] = np.log(df[EXPOSURE_COL])\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# ── 3. Find Optimal Tweedie Power ─────────────────────────────────────────────\n",
    "def find_optimal_power(\n",
    "    X: np.ndarray,\n",
    "    y: np.ndarray,\n",
    "    sample_weight: np.ndarray,\n",
    "    power_grid: list[float] = None,\n",
    "    cv: int = 5,\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Grid-search over Tweedie power parameter using cross-validated deviance.\n",
    "    1 < p < 2 is the compound Poisson-Gamma range (correct for pure premium).\n",
    "    \"\"\"\n",
    "    if power_grid is None:\n",
    "        power_grid = np.arange(1.1, 2.0, 0.1).tolist()\n",
    "\n",
    "    scores = {}\n",
    "    kf = KFold(n_splits=cv, shuffle=True, random_state=42)\n",
    "\n",
    "    for p in power_grid:\n",
    "        model = TweedieRegressor(power=p, alpha=0.1, link=\"log\", max_iter=1000)\n",
    "        fold_scores = []\n",
    "        for train_idx, val_idx in kf.split(X):\n",
    "            X_tr, X_val = X[train_idx], X[val_idx]\n",
    "            y_tr, y_val = y[train_idx], y[val_idx]\n",
    "            w_tr        = sample_weight[train_idx]\n",
    "            model.fit(X_tr, y_tr, sample_weight=w_tr)\n",
    "            preds = model.predict(X_val)\n",
    "            # Tweedie deviance — lower is better\n",
    "            score = mean_tweedie_deviance(y_val, preds, power=p)\n",
    "            fold_scores.append(score)\n",
    "        scores[p] = np.mean(fold_scores)\n",
    "        print(f\"  power={p:.1f}  mean deviance={scores[p]:.6f}\")\n",
    "\n",
    "    best_power = min(scores, key=scores.get)\n",
    "    print(f\"\\n  ✓ Best power: {best_power:.1f}  (deviance={scores[best_power]:.6f})\")\n",
    "    return best_power\n",
    "\n",
    "\n",
    "# ── 4. sklearn Pipeline ───────────────────────────────────────────────────────\n",
    "def build_sklearn_pipeline(power: float) -> Pipeline:\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            (\"num\", StandardScaler(), NUMERIC_FEATURES),\n",
    "            (\"cat\", OneHotEncoder(drop=\"first\", handle_unknown=\"ignore\"), CATEGORICAL_FEATURES),\n",
    "        ]\n",
    "    )\n",
    "    pipeline = Pipeline([\n",
    "        (\"preprocessor\", preprocessor),\n",
    "        (\"glm\", TweedieRegressor(power=power, alpha=0.1, link=\"log\", max_iter=1000)),\n",
    "    ])\n",
    "    return pipeline\n",
    "\n",
    "\n",
    "# ── 5. statsmodels GLM (with exposure offset + significance tests) ────────────\n",
    "def fit_statsmodels_glm(df: pd.DataFrame, power: float):\n",
    "    \"\"\"\n",
    "    Fits a proper actuarial Tweedie GLM with log(exposure) offset.\n",
    "    statsmodels gives AIC, BIC, p-values, and confidence intervals.\n",
    "    \"\"\"\n",
    "    # One-hot encode categoricals manually for statsmodels\n",
    "    df_model = pd.get_dummies(\n",
    "        df[NUMERIC_FEATURES + CATEGORICAL_FEATURES + [TARGET, \"log_exposure\"]],\n",
    "        columns=CATEGORICAL_FEATURES,\n",
    "        drop_first=True,\n",
    "    )\n",
    "\n",
    "    feature_cols = [c for c in df_model.columns if c not in [TARGET, \"log_exposure\"]]\n",
    "    X = sm.add_constant(df_model[feature_cols].astype(float))\n",
    "    y = df_model[TARGET].astype(float)\n",
    "    offset = df_model[\"log_exposure\"].astype(float)\n",
    "\n",
    "    # Tweedie: var_power maps to sklearn power\n",
    "    # p=1.5 → Tweedie, link=log\n",
    "    glm = sm.GLM(\n",
    "        y,\n",
    "        X,\n",
    "        family=sm.families.Tweedie(\n",
    "            link=sm.families.links.Log(),\n",
    "            var_power=power,\n",
    "        ),\n",
    "        offset=offset,\n",
    "    )\n",
    "    result = glm.fit()\n",
    "    return result\n",
    "\n",
    "\n",
    "# ── 6. Evaluation ─────────────────────────────────────────────────────────────\n",
    "def evaluate(y_true, y_pred, exposure, power: float, label: str = \"\"):\n",
    "    pp_true = y_true / exposure\n",
    "    pp_pred = y_pred / exposure\n",
    "\n",
    "    mae  = mean_absolute_error(pp_true, pp_pred)\n",
    "    dev  = mean_tweedie_deviance(y_true, y_pred, power=power)\n",
    "    # Gini-style: actual vs predicted rank correlation\n",
    "    gini = np.corrcoef(pp_true, pp_pred)[0, 1]\n",
    "\n",
    "    print(f\"\\n{'─'*40}\")\n",
    "    print(f\"  Evaluation: {label}\")\n",
    "    print(f\"  MAE (pure premium):       {mae:,.2f}\")\n",
    "    print(f\"  Tweedie Deviance:         {dev:.6f}\")\n",
    "    print(f\"  Pearson corr (pp):        {gini:.4f}\")\n",
    "    print(f\"{'─'*40}\")\n",
    "    return {\"mae\": mae, \"deviance\": dev, \"correlation\": gini}\n",
    "\n",
    "\n",
    "def plot_actual_vs_predicted(y_true, y_pred, exposure, label=\"\"):\n",
    "    pp_true = y_true / exposure\n",
    "    pp_pred = y_pred / exposure\n",
    "    plt.figure(figsize=(7, 5))\n",
    "    plt.scatter(pp_true, pp_pred, alpha=0.3, s=10)\n",
    "    lim = max(pp_true.max(), pp_pred.max())\n",
    "    plt.plot([0, lim], [0, lim], \"r--\", label=\"Perfect fit\")\n",
    "    plt.xlabel(\"Actual Pure Premium\")\n",
    "    plt.ylabel(\"Predicted Pure Premium\")\n",
    "    plt.title(f\"Actual vs Predicted Pure Premium — {label}\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# ── 7. Main ───────────────────────────────────────────────────────────────────\n",
    "def run_tweedie_glm(df: pd.DataFrame):\n",
    "    print(\"── Step 1: Preparing data ──────────────────────────────────────────\")\n",
    "    df = prepare_data(df)\n",
    "    print(f\"  Rows after cleaning: {len(df):,}\")\n",
    "\n",
    "    # Train/test split — stratify by zero vs non-zero claims\n",
    "    df[\"_zero\"] = (df[TARGET] == 0).astype(int)\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    train_df, test_df = train_test_split(df, test_size=0.2, random_state=42, stratify=df[\"_zero\"])\n",
    "    df.drop(columns=\"_zero\", inplace=True)\n",
    "    train_df = train_df.drop(columns=\"_zero\")\n",
    "    test_df  = test_df.drop(columns=\"_zero\")\n",
    "    print(f\"  Train: {len(train_df):,}  |  Test: {len(test_df):,}\")\n",
    "\n",
    "    # Encode for sklearn\n",
    "    all_features = NUMERIC_FEATURES + CATEGORICAL_FEATURES\n",
    "    X_train = train_df[all_features]\n",
    "    X_test  = test_df[all_features]\n",
    "    y_train = train_df[TARGET].values\n",
    "    y_test  = test_df[TARGET].values\n",
    "    w_train = train_df[EXPOSURE_COL].values  # exposure as sample weight\n",
    "    w_test  = test_df[EXPOSURE_COL].values\n",
    "\n",
    "    print(\"\\n── Step 2: Finding optimal Tweedie power ───────────────────────────\")\n",
    "    # Preprocess first for power search\n",
    "    preprocessor = ColumnTransformer([\n",
    "        (\"num\", StandardScaler(), NUMERIC_FEATURES),\n",
    "        (\"cat\", OneHotEncoder(drop=\"first\", handle_unknown=\"ignore\"), CATEGORICAL_FEATURES),\n",
    "    ])\n",
    "    X_train_t = preprocessor.fit_transform(X_train)\n",
    "    X_test_t  = preprocessor.transform(X_test)\n",
    "    best_power = find_optimal_power(X_train_t, y_train, sample_weight=w_train)\n",
    "\n",
    "    print(\"\\n── Step 3: Fitting sklearn Tweedie GLM ─────────────────────────────\")\n",
    "    pipeline = build_sklearn_pipeline(power=best_power)\n",
    "    pipeline.fit(X_train, y_train, glm__sample_weight=w_train)\n",
    "\n",
    "    train_preds = pipeline.predict(X_train)\n",
    "    test_preds  = pipeline.predict(X_test)\n",
    "\n",
    "    evaluate(y_train, train_preds, w_train, best_power, label=\"Train (sklearn)\")\n",
    "    evaluate(y_test,  test_preds,  w_test,  best_power, label=\"Test  (sklearn)\")\n",
    "    plot_actual_vs_predicted(y_test, test_preds, w_test, label=\"sklearn\")\n",
    "\n",
    "    print(\"\\n── Step 4: Fitting statsmodels GLM (with offset) ───────────────────\")\n",
    "    sm_result = fit_statsmodels_glm(train_df, power=best_power)\n",
    "    print(sm_result.summary())\n",
    "\n",
    "    return {\n",
    "        \"sklearn_pipeline\": pipeline,\n",
    "        \"statsmodels_result\": sm_result,\n",
    "        \"best_power\": best_power,\n",
    "        \"test_metrics\": evaluate(y_test, test_preds, w_test, best_power, label=\"Final Test\"),\n",
    "    }\n",
    "\n",
    "\n",
    "# ── Run ───────────────────────────────────────────────────────────────────────\n",
    "results = run_tweedie_glm(df)  # replace `df` with your actual DataFrame"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
