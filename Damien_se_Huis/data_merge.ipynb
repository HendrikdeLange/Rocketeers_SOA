{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef316343",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow.parquet as pq\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import TweedieRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_tweedie_deviance\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ce8f783",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_freq = pq.read_table(\"clean_business_claims_freq.parquet\").to_pandas()\n",
    "df_sev = pq.read_table(\"clean_business_claims_sev.parquet\").to_pandas()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb717fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_freq.sort_values(by=\"solar_system\", inplace=True)\n",
    "df_sev.sort_values(by=\"solar_system\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e49d03b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Grouping sev dataset based on policy id and summing claim_amount\n",
    "agg_dict = {col: 'first' for col in df_sev.columns if col not in ['claim_id', 'claim_seq', 'policy_id', 'claim_amount']}\n",
    "agg_dict['claim_amount'] = 'sum'\n",
    "\n",
    "df_grouped_business_claims_sev = (\n",
    "    df_sev.drop(columns=['claim_id', 'claim_seq'])\n",
    "    .groupby('policy_id', as_index=False)\n",
    "    .agg(agg_dict)\n",
    ")\n",
    "\n",
    "df_grouped_business_claims_sev.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ea51820",
   "metadata": {},
   "outputs": [],
   "source": [
    "#merging the two datasets on policy_id\n",
    "sev_claim_amounts = df_sev.groupby('policy_id', as_index=False)['claim_amount'].sum()\n",
    "\n",
    "df_combined = df_freq.merge(\n",
    "    sev_claim_amounts, \n",
    "    on='policy_id', \n",
    "    how='left'\n",
    ")\n",
    "\n",
    "df_combined['claim_amount'] = df_combined['claim_amount'].fillna(0)\n",
    "\n",
    "df_combined.columns\n",
    "zero_ratio = (df_combined['claim_amount'] == 0).mean()\n",
    "print(f\"Zero claims: {zero_ratio:.1%}\")\n",
    "\n",
    "#df_combined.to_csv(\"bussiness_clams_model_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df4cd73d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ── 1. Feature Schema ─────────────────────────────────────────────────────────\n",
    "NUMERIC_FEATURES = [\n",
    "    \"production_load\",\n",
    "    \"energy_backup_score\",\n",
    "    \"supply_chain_index\",\n",
    "    \"avg_crew_exp\",\n",
    "    \"maintenance_freq\",\n",
    "    \"safety_compliance\",\n",
    "]\n",
    "\n",
    "CATEGORICAL_FEATURES = [\n",
    "    \"station_id\",\n",
    "    \"solar_system\",\n",
    "]\n",
    "\n",
    "TARGET          = \"claim_amount\"\n",
    "EXPOSURE_COL    = \"exposure\"\n",
    "\n",
    "\n",
    "# ── 2. Data Prep ──────────────────────────────────────────────────────────────\n",
    "def prepare_data(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "\n",
    "    # Safety: drop rows with non-positive exposure or negative claims\n",
    "    df = df[df[EXPOSURE_COL] > 0]\n",
    "    df = df[df[TARGET] >= 0]\n",
    "\n",
    "    # Pure premium (used for evaluation, not model input)\n",
    "    df[\"pure_premium\"] = df[TARGET] / df[EXPOSURE_COL]\n",
    "\n",
    "    # Log-offset for GLM\n",
    "    df[\"log_exposure\"] = np.log(df[EXPOSURE_COL])\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# ── 3. Find Optimal Tweedie Power ─────────────────────────────────────────────\n",
    "def find_optimal_power(\n",
    "    X: np.ndarray,\n",
    "    y: np.ndarray,\n",
    "    sample_weight: np.ndarray,\n",
    "    power_grid: list[float] = None,\n",
    "    cv: int = 5,\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Grid-search over Tweedie power parameter using cross-validated deviance.\n",
    "    1 < p < 2 is the compound Poisson-Gamma range (correct for pure premium).\n",
    "    \"\"\"\n",
    "    if power_grid is None:\n",
    "        power_grid = np.arange(1.1, 2.0, 0.1).tolist()\n",
    "\n",
    "    scores = {}\n",
    "    kf = KFold(n_splits=cv, shuffle=True, random_state=42)\n",
    "\n",
    "    for p in power_grid:\n",
    "        model = TweedieRegressor(power=p, alpha=0.1, link=\"log\", max_iter=1000)\n",
    "        fold_scores = []\n",
    "        for train_idx, val_idx in kf.split(X):\n",
    "            X_tr, X_val = X[train_idx], X[val_idx]\n",
    "            y_tr, y_val = y[train_idx], y[val_idx]\n",
    "            w_tr        = sample_weight[train_idx]\n",
    "            model.fit(X_tr, y_tr, sample_weight=w_tr)\n",
    "            preds = model.predict(X_val)\n",
    "            # Tweedie deviance — lower is better\n",
    "            score = mean_tweedie_deviance(y_val, preds, power=p)\n",
    "            fold_scores.append(score)\n",
    "        scores[p] = np.mean(fold_scores)\n",
    "        print(f\"  power={p:.1f}  mean deviance={scores[p]:.6f}\")\n",
    "\n",
    "    best_power = min(scores, key=scores.get)\n",
    "    print(f\"\\n  ✓ Best power: {best_power:.1f}  (deviance={scores[best_power]:.6f})\")\n",
    "    return best_power\n",
    "\n",
    "\n",
    "# ── 4. sklearn Pipeline ───────────────────────────────────────────────────────\n",
    "def build_sklearn_pipeline(power: float) -> Pipeline:\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            (\"num\", StandardScaler(), NUMERIC_FEATURES),\n",
    "            (\"cat\", OneHotEncoder(drop=\"first\", handle_unknown=\"ignore\"), CATEGORICAL_FEATURES),\n",
    "        ]\n",
    "    )\n",
    "    pipeline = Pipeline([\n",
    "        (\"preprocessor\", preprocessor),\n",
    "        (\"glm\", TweedieRegressor(power=power, alpha=0.1, link=\"log\", max_iter=1000)),\n",
    "    ])\n",
    "    return pipeline\n",
    "\n",
    "\n",
    "# ── 5. statsmodels GLM (with exposure offset + significance tests) ────────────\n",
    "def fit_statsmodels_glm(df: pd.DataFrame, power: float):\n",
    "    \"\"\"\n",
    "    Fits a proper actuarial Tweedie GLM with log(exposure) offset.\n",
    "    statsmodels gives AIC, BIC, p-values, and confidence intervals.\n",
    "    \"\"\"\n",
    "    # One-hot encode categoricals manually for statsmodels\n",
    "    df_model = pd.get_dummies(\n",
    "        df[NUMERIC_FEATURES + CATEGORICAL_FEATURES + [TARGET, \"log_exposure\"]],\n",
    "        columns=CATEGORICAL_FEATURES,\n",
    "        drop_first=True,\n",
    "    )\n",
    "\n",
    "    feature_cols = [c for c in df_model.columns if c not in [TARGET, \"log_exposure\"]]\n",
    "    X = sm.add_constant(df_model[feature_cols].astype(float))\n",
    "    y = df_model[TARGET].astype(float)\n",
    "    offset = df_model[\"log_exposure\"].astype(float)\n",
    "\n",
    "    # Tweedie: var_power maps to sklearn power\n",
    "    # p=1.5 → Tweedie, link=log\n",
    "    glm = sm.GLM(\n",
    "        y,\n",
    "        X,\n",
    "        family=sm.families.Tweedie(\n",
    "            link=sm.families.links.Log(),\n",
    "            var_power=power,\n",
    "        ),\n",
    "        offset=offset,\n",
    "    )\n",
    "    result = glm.fit()\n",
    "    return result\n",
    "\n",
    "\n",
    "# ── 6. Evaluation ─────────────────────────────────────────────────────────────\n",
    "def evaluate(y_true, y_pred, exposure, power: float, label: str = \"\"):\n",
    "    pp_true = y_true / exposure\n",
    "    pp_pred = y_pred / exposure\n",
    "\n",
    "    mae  = mean_absolute_error(pp_true, pp_pred)\n",
    "    dev  = mean_tweedie_deviance(y_true, y_pred, power=power)\n",
    "    # Gini-style: actual vs predicted rank correlation\n",
    "    gini = np.corrcoef(pp_true, pp_pred)[0, 1]\n",
    "\n",
    "    print(f\"\\n{'─'*40}\")\n",
    "    print(f\"  Evaluation: {label}\")\n",
    "    print(f\"  MAE (pure premium):       {mae:,.2f}\")\n",
    "    print(f\"  Tweedie Deviance:         {dev:.6f}\")\n",
    "    print(f\"  Pearson corr (pp):        {gini:.4f}\")\n",
    "    print(f\"{'─'*40}\")\n",
    "    return {\"mae\": mae, \"deviance\": dev, \"correlation\": gini}\n",
    "\n",
    "\n",
    "def plot_actual_vs_predicted(y_true, y_pred, exposure, label=\"\"):\n",
    "    pp_true = y_true / exposure\n",
    "    pp_pred = y_pred / exposure\n",
    "    plt.figure(figsize=(7, 5))\n",
    "    plt.scatter(pp_true, pp_pred, alpha=0.3, s=10)\n",
    "    lim = max(pp_true.max(), pp_pred.max())\n",
    "    plt.plot([0, lim], [0, lim], \"r--\", label=\"Perfect fit\")\n",
    "    plt.xlabel(\"Actual Pure Premium\")\n",
    "    plt.ylabel(\"Predicted Pure Premium\")\n",
    "    plt.title(f\"Actual vs Predicted Pure Premium — {label}\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# ── 7. Main ───────────────────────────────────────────────────────────────────\n",
    "def run_tweedie_glm(df: pd.DataFrame):\n",
    "    print(\"── Step 1: Preparing data ──────────────────────────────────────────\")\n",
    "    df = prepare_data(df)\n",
    "    print(f\"  Rows after cleaning: {len(df):,}\")\n",
    "\n",
    "    # Train/test split — stratify by zero vs non-zero claims\n",
    "    df[\"_zero\"] = (df[TARGET] == 0).astype(int)\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    train_df, test_df = train_test_split(df, test_size=0.2, random_state=42, stratify=df[\"_zero\"])\n",
    "    df.drop(columns=\"_zero\", inplace=True)\n",
    "    train_df = train_df.drop(columns=\"_zero\")\n",
    "    test_df  = test_df.drop(columns=\"_zero\")\n",
    "    print(f\"  Train: {len(train_df):,}  |  Test: {len(test_df):,}\")\n",
    "\n",
    "    # Encode for sklearn\n",
    "    all_features = NUMERIC_FEATURES + CATEGORICAL_FEATURES\n",
    "    X_train = train_df[all_features]\n",
    "    X_test  = test_df[all_features]\n",
    "    y_train = train_df[TARGET].values\n",
    "    y_test  = test_df[TARGET].values\n",
    "    w_train = train_df[EXPOSURE_COL].values  # exposure as sample weight\n",
    "    w_test  = test_df[EXPOSURE_COL].values\n",
    "\n",
    "    print(\"\\n── Step 2: Finding optimal Tweedie power ───────────────────────────\")\n",
    "    # Preprocess first for power search\n",
    "    preprocessor = ColumnTransformer([\n",
    "        (\"num\", StandardScaler(), NUMERIC_FEATURES),\n",
    "        (\"cat\", OneHotEncoder(drop=\"first\", handle_unknown=\"ignore\"), CATEGORICAL_FEATURES),\n",
    "    ])\n",
    "    X_train_t = preprocessor.fit_transform(X_train)\n",
    "    X_test_t  = preprocessor.transform(X_test)\n",
    "    best_power = find_optimal_power(X_train_t, y_train, sample_weight=w_train)\n",
    "\n",
    "    print(\"\\n── Step 3: Fitting sklearn Tweedie GLM ─────────────────────────────\")\n",
    "    pipeline = build_sklearn_pipeline(power=best_power)\n",
    "    pipeline.fit(X_train, y_train, glm__sample_weight=w_train)\n",
    "\n",
    "    train_preds = pipeline.predict(X_train)\n",
    "    test_preds  = pipeline.predict(X_test)\n",
    "\n",
    "    evaluate(y_train, train_preds, w_train, best_power, label=\"Train (sklearn)\")\n",
    "    evaluate(y_test,  test_preds,  w_test,  best_power, label=\"Test  (sklearn)\")\n",
    "    plot_actual_vs_predicted(y_test, test_preds, w_test, label=\"sklearn\")\n",
    "\n",
    "    print(\"\\n── Step 4: Fitting statsmodels GLM (with offset) ───────────────────\")\n",
    "    sm_result = fit_statsmodels_glm(train_df, power=best_power)\n",
    "    print(sm_result.summary())\n",
    "\n",
    "    return {\n",
    "        \"sklearn_pipeline\": pipeline,\n",
    "        \"statsmodels_result\": sm_result,\n",
    "        \"best_power\": best_power,\n",
    "        \"test_metrics\": evaluate(y_test, test_preds, w_test, best_power, label=\"Final Test\"),\n",
    "    }\n",
    "\n",
    "\n",
    "# ── Run ───────────────────────────────────────────────────────────────────────\n",
    "results = run_tweedie_glm(df_combined)  # replace `df` with your actual DataFrame"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
